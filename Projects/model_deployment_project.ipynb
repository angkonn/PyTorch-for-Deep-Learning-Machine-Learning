{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VxQT-jwEvyH"
      },
      "source": [
        "# 09. PyTorch Model Deployment\n",
        "\n",
        "Welcome to Milestone Project 3: PyTorch Model Deployment!\n",
        "\n",
        "We've come a long way with our FoodVision Mini project.\n",
        "\n",
        "But so far our PyTorch models have only been accessible to us.\n",
        "\n",
        "How about we bring FoodVision Mini to life and make it publically accessible?\n",
        "\n",
        "In other words, **we're going to deploy our FoodVision Mini model to the internet as a usable app!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrFF2NiXMcNi"
      },
      "source": [
        "## What is machine learning model deployment?\n",
        "\n",
        "**Machine learning model deployment** is the process of making your machine learning model accessible to someone or something else.\n",
        "\n",
        "Someone else being a person who can interact with your model in some way.\n",
        "\n",
        "For example, someone taking a photo on their smartphone of food and then having our FoodVision Mini model classify it into pizza, steak or sushi.\n",
        "\n",
        "Something else might be another program, app or even another model that interacts with your machine learning model(s).\n",
        "\n",
        "For example, a banking database might rely on a machine learning model making predictions as to whether a transaction is fraudulent or not before transferring funds.\n",
        "\n",
        "Or an operating system may lower its resource consumption based on a machine learning model making predictions on how much power someone generally uses at specific times of day.\n",
        "\n",
        "These use cases can be mixed and matched as well.\n",
        "\n",
        "For example, a Tesla car's computer vision system will interact with the car's route planning program (something else) and then the route planning program will get inputs and feedback from the driver (someone else).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-what-is-model-deployment-someone-or-something-else.png\" width=900 alt=\"two use cases for model deployment, making your model available to someone else, for example, someone using it in an app, or making it available to something else such as another program or model\"/>\n",
        "\n",
        "*Machine learning model deployment involves making your model available to someone or something else. For example, someone might use your model as part of a food recognition app (such as FoodVision Mini or [Nutrify](https://nutrify.app)). And something else might be another model or program using your model such as a banking system using a machine learning model to detect if a transaction is fraud or not.*\n",
        "          \n",
        "          \n",
        "          "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvLh22xgNs-h"
      },
      "source": [
        "### How's it going to function?\n",
        "\n",
        "Back to the ideal use case, when you deploy your machine learning model, how should it work?\n",
        "\n",
        "As in, would you like predictions returned immediately?\n",
        "\n",
        "Or is it okay for them to happen later?\n",
        "\n",
        "These two scenarios are generally referred to as:\n",
        "\n",
        "* **Online (real-time)** - Predictions/inference happen **immediately**. For example, someone uploads an image, the image gets transformed and predictions are returned or someone makes a purchase and the transaction is verified to be non-fraudulent by a model so the purchase can go through.\n",
        "* **Offline (batch)** - Predictions/inference happen **periodically**. For example, a photos application sorts your images into different categories (such as beach, mealtime, family, friends) whilst your mobile device is plugged into charge.\n",
        "\n",
        "> **Note:** \"Batch\" refers to inference being performed on multiple samples at a time. However, to add a little confusion, batch processing can happen immediately/online (multiple images being classified at once) and/or offline (multiple images being predicted/trained on at once).  \n",
        "\n",
        "The main difference between each being: predictions being made immediately or periodically.\n",
        "\n",
        "Periodically can have a varying timescale too, from every few seconds to every few hours or days.\n",
        "\n",
        "And you can mix and match the two.\n",
        "\n",
        "In the case of FoodVision Mini, we'd want our inference pipeline to happen online (real-time), so when someone uploads an image of pizza, steak or sushi, the prediction results are returned immediately (any slower than real-time would make a boring experience).\n",
        "\n",
        "But for our training pipeline, it's okay for it to happen in a batch (offline) fashion, which is what we've been doing throughout the previous chapters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtmigCDcOSJF"
      },
      "source": [
        "### How's it going to function?\n",
        "\n",
        "Back to the ideal use case, when you deploy your machine learning model, how should it work?\n",
        "\n",
        "As in, would you like predictions returned immediately?\n",
        "\n",
        "Or is it okay for them to happen later?\n",
        "\n",
        "These two scenarios are generally referred to as:\n",
        "\n",
        "* **Online (real-time)** - Predictions/inference happen **immediately**. For example, someone uploads an image, the image gets transformed and predictions are returned or someone makes a purchase and the transaction is verified to be non-fraudulent by a model so the purchase can go through.\n",
        "* **Offline (batch)** - Predictions/inference happen **periodically**. For example, a photos application sorts your images into different categories (such as beach, mealtime, family, friends) whilst your mobile device is plugged into charge.\n",
        "\n",
        "> **Note:** \"Batch\" refers to inference being performed on multiple samples at a time. However, to add a little confusion, batch processing can happen immediately/online (multiple images being classified at once) and/or offline (multiple images being predicted/trained on at once).  \n",
        "\n",
        "The main difference between each being: predictions being made immediately or periodically.\n",
        "\n",
        "Periodically can have a varying timescale too, from every few seconds to every few hours or days.\n",
        "\n",
        "And you can mix and match the two.\n",
        "\n",
        "In the case of FoodVision Mini, we'd want our inference pipeline to happen online (real-time), so when someone uploads an image of pizza, steak or sushi, the prediction results are returned immediately (any slower than real-time would make a boring experience).\n",
        "\n",
        "But for our training pipeline, it's okay for it to happen in a batch (offline) fashion, which is what we've been doing throughout the previous chapters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTmhYMHdOZVV"
      },
      "source": [
        "### Ways to deploy a machine learning model\n",
        "\n",
        "We've discussed a couple of options for deploying machine learning models (on-device and cloud).\n",
        "\n",
        "And each of these will have their specific requirements:\n",
        "\n",
        "| **Tool/resource** | **Deployment type** |\n",
        "| ----- | ----- |\n",
        "| [Google's ML Kit](https://developers.google.com/ml-kit) | On-device (Android and iOS) |\n",
        "| [Apple's Core ML](https://developer.apple.com/documentation/coreml) and [`coremltools` Python package](https://coremltools.readme.io/docs) | On-device (all Apple devices) |\n",
        "| [Amazon Web Service's (AWS) Sagemaker](https://aws.amazon.com/sagemaker/) | Cloud |\n",
        "| [Google Cloud's Vertex AI](https://cloud.google.com/vertex-ai) | Cloud |\n",
        "| [Microsoft's Azure Machine Learning](https://azure.microsoft.com/en-au/services/machine-learning/) | Cloud |\n",
        "| [Hugging Face Spaces](https://huggingface.co/spaces) | Cloud |\n",
        "| API with [FastAPI](https://fastapi.tiangolo.com) | Cloud/self-hosted server |\n",
        "| API with [TorchServe](https://pytorch.org/serve/) | Cloud/self-hosted server |\n",
        "| [ONNX (Open Neural Network Exchange)](https://onnx.ai/index.html) | Many/general |\n",
        "| Many more... ||\n",
        "\n",
        "> **Note:** An [application programming interface (API)](https://en.wikipedia.org/wiki/API) is a way for two (or more) computer programs to interact with each other. For example, if your model was deployed as API, you would be able to write a program that could send data to it and then receive predictions back.\n",
        "\n",
        "Which option you choose will be highly dependent on what you're building/who you're working with.\n",
        "\n",
        "But with so many options, it can be very intimidating.\n",
        "\n",
        "So best to start small and keep it simple.\n",
        "\n",
        "And one of the best ways to do so is by turning your machine learning model into a demo app with [Gradio](https://gradio.app) and then deploying it on Hugging Face Spaces.\n",
        "\n",
        "We'll be doing just that with FoodVision Mini later on.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-tools-and-places-to-deploy-ml-models.png\" alt=\"tools and places to deploy machine learning models\" width=900/>\n",
        "\n",
        "*A handful of places and tools to host and deploy machine learning models. There are plenty I've missed so if you'd like to add more, please leave a [discussion on GitHub](https://github.com/mrdbourke/pytorch-deep-learning/discussions).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvmTh90TOnRY"
      },
      "source": [
        "## What we're going to cover\n",
        "\n",
        "Enough talking about deploying a machine learning model.\n",
        "\n",
        "Let's become machine learning engineers and actually deploy one.\n",
        "\n",
        "Our goal is to deploy our FoodVision Model via a demo Gradio app with the following metrics:\n",
        "1. **Performance:** 95%+ accuracy.\n",
        "2. **Speed:** real-time inference of 30FPS+ (each prediction has a latency of lower than ~0.03s).\n",
        "\n",
        "We'll start by running an experiment to compare our best two models so far: EffNetB2 and ViT feature extractors.\n",
        "\n",
        "Then we'll deploy the one which performs closest to our goal metrics.\n",
        "\n",
        "Finally, we'll finish with a (BIG) surprise bonus.\n",
        "\n",
        "| **Topic** | **Contents** |\n",
        "| ----- | ----- |\n",
        "| **0. Getting setup** | We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. |\n",
        "| **1. Get data** | Let's download the [`pizza_steak_sushi_20_percent.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) dataset so we can train our previously best performing models on the same dataset. |\n",
        "| **2. FoodVision Mini model deployment experiment outline** | Even on the third milestone project, we're still going to be running multiple experiments to see which model (EffNetB2 or ViT) achieves closest to our goal metrics. |\n",
        "| **3. Creating an EffNetB2 feature extractor** | An EfficientNetB2 feature extractor performed the best on our pizza, steak, sushi dataset in [07. PyTorch Experiment Tracking](https://www.learnpytorch.io/07_pytorch_experiment_tracking/), let's recreate it as a candidate for deployment. |\n",
        "| **4. Creating a ViT feature extractor** | A ViT feature extractor has been the best performing model yet on our pizza, steak, sushi dataset in [08. PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/), let's recreate it as a candidate for deployment alongside EffNetB2. |\n",
        "| **5. Making predictions with our trained models and timing them** | We've built two of the best performing models yet, let's make predictions with them and track their results. |\n",
        "| **6. Comparing model results, prediction times and size** | Let's compare our models to see which performs best with our goals. |\n",
        "| **7. Bringing FoodVision Mini to life by creating a Gradio demo** | One of our models performs better than the other (in terms of our goals), so let's turn it into a working app demo! |\n",
        "| **8. Turning our FoodVision Mini Gradio demo into a deployable app** | Our Gradio app demo works locally, let's prepare it for deployment! |\n",
        "| **9. Deploying our Gradio demo to HuggingFace Spaces** | Let's take FoodVision Mini to the web and make it pubically accessible for all! |\n",
        "| **10. Creating a BIG surprise** | We've built FoodVision Mini, time to step things up a notch. |\n",
        "| **11. Deploying our BIG surprise** | Deploying one app was fun, how about we make it two? |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1S3pnTlwX5y"
      },
      "source": [
        "## 0. Getting setup\n",
        "\n",
        "As we've done previously, let's make sure we've got all of the modules we'll need for this section.\n",
        "\n",
        "We'll import the Python scripts (such as `data_setup.py` and `engine.py`) we created in [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n",
        "\n",
        "To do so, we'll download [`going_modular`](https://github.com/angkonn/PyTorch-for-Deep-Learning-Machine-Learning/tree/main/going_modular) directory from the [`pytorch-deep-learning` repository](https://github.com/angkonn/PyTorch-for-Deep-Learning-Machine-Learning/tree/main) (if we don't already have it).\n",
        "\n",
        "We'll also get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available.\n",
        "\n",
        "`torchinfo` will help later on to give us a visual representation of our model.\n",
        "\n",
        "And since later on we'll be using `torchvision` v0.13 package (available as of July 2022), we'll make sure we've got the latest versions.\n",
        "\n",
        "> **Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJUDJXPnwtXj",
        "outputId": "543d2296-c00e-4dfd-c707-d8df11d0978d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "torch version: 2.6.0+cu124\n",
            "torchvision version: 0.21.0+cu124\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N_kmxXpSxBCk"
      },
      "outputs": [],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/angkonn/PyTorch-for-Deep-Learning-Machine-Learning\n",
        "    !mv PyTorch-for-Deep-Learning-Machine-Learning/going_modular .\n",
        "\n",
        "    !mv PyTorch-for-Deep-Learning-Machine-Learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf PyTorch-for-Deep-Learning-Machine-Learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mMRc3mOVwuIS",
        "outputId": "26e2fd6b-e43d-47b7-cdfc-2b496c7816b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device =\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eNqEULMwuFq"
      },
      "source": [
        "## 1. Getting data\n",
        "\n",
        "We left off in [08. PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size) comparing our own Vision Transformer (ViT) feature extractor model to the EfficientNetB2 (EffNetB2) feature extractor model we created in [07. PyTorch Experiment Tracking](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it).\n",
        "\n",
        "And we found that there was a slight difference in the comparison.\n",
        "\n",
        "The EffNetB2 model was trained on 20% of the pizza, steak and sushi data from Food101 where as the ViT model was trained on 10%.\n",
        "\n",
        "Since our goal is to deploy the best model for our FoodVision Mini problem, let's start by downloading the [20% pizza, steak and sushi dataset](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) and train an EffNetB2 feature extractor and ViT feature extractor on it and then compare the two models.\n",
        "\n",
        "This way we'll be comparing apples to apples (one model trained on a dataset to another model trained on the same dataset).\n",
        "\n",
        "> **Note:** The dataset we're downloading is a sample of the entire [Food101 dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html#food101) (101 food classes with 1,000 images each). More specifically, 20% refers to 20% of images from the pizza, steak and sushi classes selected at random. You can see how this dataset was created in [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) and more details in [04. PyTorch Custom Datasets section 1](https://www.learnpytorch.io/04_pytorch_custom_datasets/#1-get-data).\n",
        "\n",
        "We can download the data using the `download_data()` function we created in [07. PyTorch Experiment Tracking section 1](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data) from [`helper_functions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDI0K8v_wuCi",
        "outputId": "4cdcae63-8224-4375-c9e4-0290759a02f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi_20_percent')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
        "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
        "\n",
        "data_20_percent_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0PXSaulwt_j",
        "outputId": "0c7aac8c-28f8-4e4b-a32d-8c152ed39f96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi_20_percent/train'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# setup directory paths to train and test images\n",
        "train_dir = data_20_percent_path / \"train\"\n",
        "test_dir = data_20_percent_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jhNVsJTwt8j"
      },
      "source": [
        "## 2. FoodVision Mini model deployment experiment outline\n",
        "\n",
        "The ideal deployed model FoodVision Mini performs well and fast.\n",
        "\n",
        "We'd like our model to perform as close to real-time as possible.\n",
        "\n",
        "Real-time in this case being ~30FPS (frames per second) because that's [about how fast the human eye can see](https://www.healthline.com/health/human-eye-fps) (there is debate on this but let's just use ~30FPS as our benchmark).\n",
        "\n",
        "And for classifying three different classes (pizza, steak and sushi), we'd like a model that performs at 95%+ accuracy.\n",
        "\n",
        "Of course, higher accuracy would be nice but this might sacrifice speed.\n",
        "\n",
        "So our goals are:\n",
        "\n",
        "1. **Performance** - A model that performs at 95%+ accuracy.\n",
        "2. **Speed** - A model that can classify an image at ~30FPS (0.03 seconds inference time per image, also known as latency).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployments-speed-vs-inference.png\" alt=\"foodvision mini goals in terms of performance and inference time.\" width=750/>\n",
        "\n",
        "*FoodVision Mini deployment goals. We'd like a fast predicting well-performing model (because a slow app is boring).*\n",
        "\n",
        "We'll put an emphasis on speed, meaning, we'd prefer a model performing at 90%+ accuracy at ~30FPS than a model performing 95%+ accuracy at 10FPS.\n",
        "\n",
        "To try and achieve these results, let's bring in our best performing models from the previous sections:\n",
        "\n",
        "1. **EffNetB2 feature extractor** (EffNetB2 for short) - originally created in [07. PyTorch Experiment Tracking section 7.5](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models) using [`torchvision.models.efficientnet_b2()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#efficientnet-b2) with adjusted `classifier` layers.\n",
        "2. **ViT-B/16 feature extractor** (ViT for short) - originally created in [08. PyTorch Paper Replicating section 10](https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset) using [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#vit-b-16) with adjusted `head` layers.\n",
        "    * **Note** ViT-B/16 stands for \"Vision Transformer Base, patch size 16\".\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-two-experiments.png\" alt=\"modelling experiments for foodvision mini deployments, one effnetb2 feature extractor model and a vision transformer feature extractor model\" width=750 />\n",
        "\n",
        "> **Note:** A \"feature extractor model\" often starts with a model that has been pretrained on a dataset similar to your own problem. The pretrained model's base layers are often left frozen (the pretrained patterns/weights stay the same) whilst some of the top (or classifier/classification head) layers get customized to your own problem by training on your own data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qDYFlYqfwt5q"
      },
      "outputs": [],
      "source": [
        "# 1. Setup pretrained EffNetB2 weights\n",
        "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "\n",
        "# 2. Get EffNetB2 transforms\n",
        "effnetb2_transforms = effnetb2_weights.transforms()\n",
        "\n",
        "# 3. Setup pretrained model\n",
        "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n",
        "\n",
        "# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\n",
        "for param in effnetb2.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgVc_dJhwt2z",
        "outputId": "f917e333-e8f4-47a2-c18c-ccfae12cbf72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Dropout(p=0.3, inplace=True)\n",
              "  (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Check out EffNetB2 classifier head\n",
        "effnetb2.classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To change the classifier head to suit our own problem, let's replace the `out_features` variable with the same number of classes we have (in our case, `out_features=3`, one for pizza, steak, sushi)."
      ],
      "metadata": {
        "id": "0814ZrzFwt0M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3D45jX_6wtxa"
      },
      "outputs": [],
      "source": [
        "# 5. Update the classifier head\n",
        "effnetb2.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n",
        "    nn.Linear(in_features=1408, # keep in_features same\n",
        "              out_features=3)) # change out_features to suit our number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FiyrG25Wwtub"
      },
      "outputs": [],
      "source": [
        "def create_effnetb2_model(num_classes:int=3,\n",
        "                          seed:int=42):\n",
        "\n",
        "    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # 4. Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 5. Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jMWEfSRkwtrj"
      },
      "outputs": [],
      "source": [
        "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n",
        "                                                      seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Print EffNetB2 model summary\n",
        "summary(effnetb2,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "mjP9UUC-iQvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c9ea16e-fcb9-4136-a69a-20e93b99baa5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 3]               --                   Partial\n",
              "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
              "│    │    └─SiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
              "│    └─Sequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
              "│    │    └─MBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
              "│    └─Sequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    │    └─MBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    └─Sequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
              "│    │    └─MBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    │    └─MBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    └─Sequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
              "│    │    └─MBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    └─Sequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
              "│    │    └─MBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    └─Sequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
              "│    │    └─MBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    └─Sequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
              "│    │    └─MBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
              "│    └─Conv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
              "│    │    └─SiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
              "├─Sequential (classifier)                                    [1, 1408]            [1, 3]               --                   True\n",
              "│    └─Dropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
              "│    └─Linear (1)                                            [1, 1408]            [1, 3]               4,227                True\n",
              "============================================================================================================================================\n",
              "Total params: 7,705,221\n",
              "Trainable params: 4,227\n",
              "Non-trainable params: 7,700,994\n",
              "Total mult-adds (Units.MEGABYTES): 657.64\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 156.80\n",
              "Params size (MB): 30.82\n",
              "Estimated Total Size (MB): 188.22\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Creating DataLoaders for EffNetB2"
      ],
      "metadata": {
        "id": "PcDNsHuGiQsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup DataLoaders\n",
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                 test_dir=test_dir,\n",
        "                                                                                                 transform=effnetb2_transforms,\n",
        "                                                                                                 batch_size=32)"
      ],
      "metadata": {
        "id": "iCPAaMYkiQpY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
        "                             lr=1e-3)\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Set seeds for reproducibility and train the model\n",
        "set_seeds()\n",
        "effnetb2_results = engine.train(model=effnetb2,\n",
        "                                train_dataloader=train_dataloader_effnetb2,\n",
        "                                test_dataloader=test_dataloader_effnetb2,\n",
        "                                epochs=10,\n",
        "                                optimizer=optimizer,\n",
        "                                loss_fn=loss_fn,\n",
        "                                device=device)"
      ],
      "metadata": {
        "id": "aTTSZDLsiQms",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "87a7876cfe054d01aa15f44dc4b93ab8",
            "86a2cc37693c4ba8a273968fc6ae5050",
            "bc3a18d069214cbe89cbe4b153e4f193",
            "5e53ffc3c15d43e885721a3f34d41d9f",
            "91a17accaf7549f6aee235674e28d9ac",
            "67a2f11c7d09408dac0d18c3cc37b064",
            "0fe2f9569f8141adbf55e635e0df9dca",
            "66ff84bc57f148c9bdf37d38cc3d4c8d",
            "bfbcccc7c5f44417906eefcc629a6f7a",
            "f2fb5c154b4e44f282ab431dbac2c663",
            "3552726fe5b540acad5daa1faa5a1612"
          ]
        },
        "outputId": "ba917474-a195-4e40-f525-38bf2668378c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87a7876cfe054d01aa15f44dc4b93ab8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.9839 | train_acc: 0.5667 | test_loss: 0.7393 | test_acc: 0.9409\n",
            "Epoch: 2 | train_loss: 0.7135 | train_acc: 0.8396 | test_loss: 0.5862 | test_acc: 0.9409\n",
            "Epoch: 3 | train_loss: 0.5874 | train_acc: 0.8958 | test_loss: 0.4891 | test_acc: 0.9563\n",
            "Epoch: 4 | train_loss: 0.4488 | train_acc: 0.9146 | test_loss: 0.4338 | test_acc: 0.9409\n",
            "Epoch: 5 | train_loss: 0.4277 | train_acc: 0.9125 | test_loss: 0.3907 | test_acc: 0.9443\n",
            "Epoch: 6 | train_loss: 0.4392 | train_acc: 0.8896 | test_loss: 0.3525 | test_acc: 0.9688\n",
            "Epoch: 7 | train_loss: 0.4246 | train_acc: 0.8771 | test_loss: 0.3263 | test_acc: 0.9563\n",
            "Epoch: 8 | train_loss: 0.3885 | train_acc: 0.8979 | test_loss: 0.3465 | test_acc: 0.9443\n",
            "Epoch: 9 | train_loss: 0.3795 | train_acc: 0.8812 | test_loss: 0.3127 | test_acc: 0.9193\n",
            "Epoch: 10 | train_loss: 0.3752 | train_acc: 0.8688 | test_loss: 0.2811 | test_acc: 0.9625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Inspecting EffNetB2 loss curve"
      ],
      "metadata": {
        "id": "ZOojRycwiQjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(effnetb2_results)"
      ],
      "metadata": {
        "id": "J3rUOwDMiQgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "c588942f-f71f-4253-c22d-e03c8c02f8d4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.5 Saving EffNetB2 feature extractor\n",
        "\n",
        "Now we've got a well-performing trained model, let's save it to file so we can import and use it later.\n",
        "\n",
        "To save our model we can use the [`utils.save_model()`](https://github.com/angkonn/PyTorch-for-Deep-Learning-Machine-Learning/blob/main/going_modular/going_modular/utils.py) function we created in [05. PyTorch Going Modular section 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy).\n",
        "\n",
        "We'll set the `target_dir` to `\"models\"` and the `model_name` to `\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"` (a little comprehensive but at least we know what's going on)."
      ],
      "metadata": {
        "id": "PYVwtaueiQd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "# Save the model\n",
        "utils.save_model(model=effnetb2,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "id": "YDqJN5qqiQbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03aaf6a1-b567-42e5-b1ec-a4a29ad699f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to: models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Checking the size of EffNetB2 feature extractor\n",
        "\n",
        "Since one of our criteria for deploying a model to power FoodVision Mini is **speed** (~30FPS or better), let's check the size of our model.\n",
        "\n",
        "Why check the size?\n",
        "\n",
        "Well, while not always the case, the size of a model can influence its inference speed.\n",
        "\n",
        "As in, if a model has more parameters, it generally performs more operations and each one of these operations requires some computing power.\n",
        "\n",
        "And because we'd like our model to work on devices with limited computing power (e.g. on a mobile device or in a web browser), generally, the smaller the size the better (as long as it still performs well in terms of accuracy).\n",
        "\n",
        "To check our model's size in bytes, we can use Python's [`pathlib.Path.stat(\"path_to_model\").st_size`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.stat) and then we can convert it (roughly) to megabytes by dividing it by `(1024*1024)`."
      ],
      "metadata": {
        "id": "R6-JcyBfiQYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")"
      ],
      "metadata": {
        "id": "YXf0Ow-fiQWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6887d4d9-6a22-4acf-cb8e-96310a9a6962"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained EffNetB2 feature extractor model size: 29 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Collecting EffNetB2 feature extractor stats"
      ],
      "metadata": {
        "id": "OcWemFZFiQTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of parameters in EffNetB2\n",
        "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
        "effnetb2_total_params"
      ],
      "metadata": {
        "id": "Ro_M-ZA9i8wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb0436c-c568-482d-9c2f-788dbaa8c3c3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7705221"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with EffNetB2 statistics\n",
        "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n",
        "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
        "                  \"number_of_parameters\": effnetb2_total_params,\n",
        "                  \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "8cCqo8L-i8uc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7bd7e53-32f9-49a7-a639-076842d5b6f4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'test_loss': 0.28108686208724976,\n",
              " 'test_acc': 0.9625,\n",
              " 'number_of_parameters': 7705221,\n",
              " 'model_size (MB)': 29}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Creating a ViT feature extractor\n",
        "\n",
        "Time to continue with our FoodVision Mini modelling experiments.\n",
        "\n",
        "This time we're going to create a ViT feature extractor.\n",
        "\n",
        "And we'll do it in much the same way as the EffNetB2 feature extractor except this time with [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16) instead of `torchvision.models.efficientnet_b2()`.\n",
        "\n",
        "We'll start by creating a function called `create_vit_model()` which will be very similar to `create_effnetb2_model()` except of course returning a ViT feature extractor model and transforms rather than EffNetB2.\n",
        "\n",
        "Another slight difference is that `torchvision.models.vit_b_16()`'s output layer is called `heads` rather than `classifier`."
      ],
      "metadata": {
        "id": "fehdAyDhi8rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out ViT heads layer\n",
        "vit = torchvision.models.vit_b_16()\n",
        "vit.heads"
      ],
      "metadata": {
        "id": "Qdz8nJHQi8pE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12fbd2e-9bd5-4ab3-a65b-c174dfa5c14b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_model(num_classes:int=3,\n",
        "                     seed:int=42):\n",
        "\n",
        "    # Create ViT_B_16 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.vit_b_16(weights=weights)\n",
        "\n",
        "    # Freeze all layers in model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head to suit our needs (this will be trainable)\n",
        "    torch.manual_seed(seed)\n",
        "    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n",
        "                                          out_features=num_classes)) # update to reflect target number of classes\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "PgCY9vwFi8mc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ViT model and transforms\n",
        "vit, vit_transforms = create_vit_model(num_classes=3,\n",
        "                                       seed=42)"
      ],
      "metadata": {
        "id": "KhsQhDcui8ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af19192d-cce5-481b-a6a5-bdfd915eac94"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:03<00:00, 92.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Print ViT feature extractor model summary\n",
        "summary(vit,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "vAtg9lwOi8hN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa5674b-f04b-49ac-b894-e688e24e60ae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 3]               768                  Partial\n",
              "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     (590,592)            False\n",
              "├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False\n",
              "│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n",
              "│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False\n",
              "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False\n",
              "├─Sequential (heads)                                         [1, 768]             [1, 3]               --                   True\n",
              "│    └─Linear (0)                                            [1, 768]             [1, 3]               2,307                True\n",
              "============================================================================================================================================\n",
              "Total params: 85,800,963\n",
              "Trainable params: 2,307\n",
              "Non-trainable params: 85,798,656\n",
              "Total mult-adds (Units.MEGABYTES): 172.47\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 104.09\n",
              "Params size (MB): 229.20\n",
              "Estimated Total Size (MB): 333.89\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Create DataLoaders for ViT"
      ],
      "metadata": {
        "id": "wXfZDMYJjNwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup ViT DataLoaders\n",
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                       test_dir=test_dir,\n",
        "                                                                                       transform=vit_transforms,\n",
        "                                                                                       batch_size=32)"
      ],
      "metadata": {
        "id": "GySBp7wEjNuG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Training ViT feature extractor"
      ],
      "metadata": {
        "id": "kcOVHzRCjNrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train ViT model with seeds set for reproducibility\n",
        "set_seeds()\n",
        "vit_results = engine.train(model=vit,\n",
        "                           train_dataloader=train_dataloader_vit,\n",
        "                           test_dataloader=test_dataloader_vit,\n",
        "                           epochs=10,\n",
        "                           optimizer=optimizer,\n",
        "                           loss_fn=loss_fn,\n",
        "                           device=device)"
      ],
      "metadata": {
        "id": "qA4DJnzsjNpA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "f7db65f779c14d8c80dd38dea3215c42",
            "5c40b25147dd42219d09257a1631ce2c",
            "e9165874ee7a451ca285afe2a72641f7",
            "658fac3786d24ff89f47fef49baa2f71",
            "3ab4f077c5c4466e864d4edb04a4145c",
            "fdcffa90dac24f358d2a31076fa055ed",
            "81bb1c23dafa46d8ba936575925993e8",
            "41998295434947b680d14db02b982371",
            "4b9435b544324446b478d57e2c016b1c",
            "523357fcff054e06b028bfc329ac6b22",
            "205e36d46196400ba4fe8da2c8a9653e"
          ]
        },
        "outputId": "d695675a-d7c2-4b0e-95c0-f958efbf3149"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7db65f779c14d8c80dd38dea3215c42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.7020 | train_acc: 0.7521 | test_loss: 0.2714 | test_acc: 0.9381\n",
            "Epoch: 2 | train_loss: 0.2532 | train_acc: 0.9062 | test_loss: 0.1672 | test_acc: 0.9602\n",
            "Epoch: 3 | train_loss: 0.1764 | train_acc: 0.9542 | test_loss: 0.1273 | test_acc: 0.9693\n",
            "Epoch: 4 | train_loss: 0.1276 | train_acc: 0.9625 | test_loss: 0.1074 | test_acc: 0.9722\n",
            "Epoch: 5 | train_loss: 0.1159 | train_acc: 0.9646 | test_loss: 0.0953 | test_acc: 0.9784\n",
            "Epoch: 6 | train_loss: 0.1274 | train_acc: 0.9375 | test_loss: 0.0832 | test_acc: 0.9722\n",
            "Epoch: 7 | train_loss: 0.0897 | train_acc: 0.9771 | test_loss: 0.0845 | test_acc: 0.9784\n",
            "Epoch: 8 | train_loss: 0.0919 | train_acc: 0.9812 | test_loss: 0.0764 | test_acc: 0.9722\n",
            "Epoch: 9 | train_loss: 0.0922 | train_acc: 0.9792 | test_loss: 0.0734 | test_acc: 0.9784\n",
            "Epoch: 10 | train_loss: 0.0658 | train_acc: 0.9833 | test_loss: 0.0644 | test_acc: 0.9847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Inspecting ViT loss curves"
      ],
      "metadata": {
        "id": "hS3MqgUBjNmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(vit_results)"
      ],
      "metadata": {
        "id": "wScK4JUEjNjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Saving ViT feature extractor\n",
        "\n",
        "Our ViT model is performing outstanding!\n",
        "\n",
        "So let's save it to file so we can import it and use it later if we wis"
      ],
      "metadata": {
        "id": "JC40fInEjNgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=vit,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "id": "uwjvINM8jcX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
      ],
      "metadata": {
        "id": "Fn7AiTJjjekQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of parameters in ViT\n",
        "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
        "vit_total_params"
      ],
      "metadata": {
        "id": "cF2Hv9THjeho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That looks like a fair bit more than our EffNetB2!"
      ],
      "metadata": {
        "id": "H5nmIO8Kjees"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ViT statistics dictionary\n",
        "vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n",
        "             \"test_acc\": vit_results[\"test_acc\"][-1],\n",
        "             \"number_of_parameters\": vit_total_params,\n",
        "             \"model_size (MB)\": pretrained_vit_model_size}\n",
        "\n",
        "vit_stats"
      ],
      "metadata": {
        "id": "GVsD5N1Fjeb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Making predictions with our trained models and timing them\n",
        "\n",
        "We've got a couple of trained models, both performing pretty well.\n",
        "\n",
        "Now how about we test them out doing what we'd like them to do?\n",
        "\n",
        "As in, let's see how they go making predictions (performing inference).\n",
        "\n",
        "We know both of our models are performing at over 95% accuracy on the test dataset, but how fast are they?\n",
        "\n",
        "Ideally, if we're deploying our FoodVision Mini model to a mobile device so people can take photos of their food and identify it, we'd like the predictions to happen at real-time (~30 frames per second).\n",
        "\n",
        "That's why our second criteria is: a fast model.\n",
        "\n",
        "To find out how long each of our models take to performance inference, let's create a function called `pred_and_store()` to iterate over each of the test dataset images one by one and perform a prediction.\n",
        "\n",
        "We'll time each of the predictions as well as store the results in a common prediction format: a list of dictionaries (where each element in the list is a single prediction and each sinlge prediction is a dictionary).\n",
        "\n",
        "> **Note:** We time the predictions one by one rather than by batch because when our model is deployed, it will likely only be making a prediction on one image at a time. As in, someone takes a photo and our model predicts on that single image.\n",
        "\n",
        "Since we'd like to make predictions across all the images in the test set, let's first get a list of all of the test image paths so we can iterate over them.\n",
        "\n",
        "To do so, we'll use Python's [`pathlib.Path(\"target_dir\").glob(\"*/*.jpg\"))`](https://docs.python.org/3/library/pathlib.html#basic-use) to find all of the filepaths in a target directory with the extension `.jpg` (all of our test images)."
      ],
      "metadata": {
        "id": "1iq73RU2jeZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get all test data paths\n",
        "print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_paths[:5]"
      ],
      "metadata": {
        "id": "suAsPcYsjeWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Creating a function to make predictions across the test dataset\n",
        "\n",
        "Now we've got a list of our test image paths, let's get to work on our `pred_and_store()` function:\n",
        "\n",
        "1. Create a function that takes a list of paths, a trained PyTorch model, a series of transforms (to prepare images), a list of target class names and a target device.\n",
        "2. Create an empty list to store prediction dictionaries (we want the function to return a list of dictionaries, one for each prediction).\n",
        "3. Loop through the target input paths (steps 4-14 will happen inside the loop).\n",
        "4. Create an empty dictionary for each iteration in the loop to store prediction values per sample.\n",
        "5. Get the sample path and ground truth class name (we can do this by inferring the class from the path).\n",
        "6. Start the prediction timer using Python's [`timeit.default_timer()`](https://docs.python.org/3/library/timeit.html#timeit.default_timer).\n",
        "7. Open the image using [`PIL.Image.open(path)`](https://pillow.readthedocs.io/en/stable/reference/Image.html#functions).\n",
        "8. Transform the image so it's capable of being used with the target model as well as add a batch dimension and send the image to the target device.\n",
        "9. Prepare the model for inference by sending it to the target device and turning on `eval()` mode.\n",
        "10. Turn on [`torch.inference_mode()`](https://pytorch.org/docs/stable/generated/torch.inference_mode.html) and pass the target transformed image to the model and calculate the prediction probability using `torch.softmax()` and the target label using `torch.argmax()`.\n",
        "11. Add the prediction probability and prediction class to the prediction dictionary created in step 4. Also make sure the prediction probability is on the CPU so it can be used with non-GPU libraries such as NumPy and pandas for later inspection.\n",
        "12. End the prediction timer started in step 6 and add the time to the prediction dictionary created in step 4.\n",
        "13. See if the predicted class matches the ground truth class from step 5 and add the result to the prediction dictionary created in step 4.\n",
        "14. Append the updated prediction dictionary to the empty list of predictions created in step 2.\n",
        "15. Return the list of prediction dictionaries.\n",
        "\n",
        "A bunch of steps, but nothing we can't handle!\n",
        "\n",
        "Let's do it."
      ],
      "metadata": {
        "id": "Qbp1WIDOjeT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict\n",
        "\n",
        "# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\n",
        "def pred_and_store(paths: List[pathlib.Path],\n",
        "                   model: torch.nn.Module,\n",
        "                   transform: torchvision.transforms,\n",
        "                   class_names: List[str],\n",
        "                   device: str = \"cuda\" if torch.cuda.is_available()\n",
        "                   else \"cpu\") -> List[Dict]:\n",
        "    # 2. Create an empty list to store prediction dictionaries\n",
        "    pred_list = []\n",
        "\n",
        "    # 3. Loop through target paths\n",
        "    for path in tqdm(paths):\n",
        "\n",
        "        # 4. Create empty dictionary to store prediction information for each sample\n",
        "        pred_dict = {}\n",
        "\n",
        "        # 5. Get sample path and ground truth class name\n",
        "        pred_dict[\"image_path\"] = path\n",
        "        class_name = path.parent.stem\n",
        "        pred_dict[\"class_name\"] = class_name\n",
        "\n",
        "        # 6. Start the prediction timer\n",
        "        start_time = timer()\n",
        "\n",
        "        # 7. Open image path\n",
        "        img = Image.open(path)\n",
        "\n",
        "        # 8. Transform the image, add batch dimension and put image on target device\n",
        "        transformed_image = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        # 10. Get prediction probability, prediction label and prediction class\n",
        "        with torch.inference_mode():\n",
        "            pred_logit = model(transformed_image)\n",
        "            pred_prob = torch.softmax(pred_logit, dim=1)\n",
        "            pred_label = torch.argmax(pred_prob, dim=1)\n",
        "            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n",
        "\n",
        "            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on)\n",
        "            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(),4)\n",
        "            pred_dict[\"pred_class\"] = pred_class\n",
        "\n",
        "            # 12. End the timer and calculate time per pred\n",
        "            end_time = timer()\n",
        "            pred_dict[\"time_for_pred\"] = round(end_time-start_time,4)\n",
        "\n",
        "        # 13. Compare prediction class to ground truth class and add to dictionary\n",
        "        pred_dict[\"correct\"] = class_name == pred_class\n",
        "\n",
        "        # 14. Add the dictionary to the list of preds\n",
        "        pred_list.append(pred_dict)\n",
        "\n",
        "    # 15. Return list of prediction dictionaries\n",
        "    return pred_list\n",
        "\n"
      ],
      "metadata": {
        "id": "LcorDnD3jeRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Making and timing predictions with EffNetB2\n",
        "\n",
        "Time to test out our `pred_and_store()` function!\n",
        "\n",
        "Let's start by using it to make predictions across the test dataset with our EffNetB2 model, paying attention to two details:\n",
        "\n",
        "1. **Device** - We'll hard code the `device` parameter to use `\"cpu\"` because when we deploy our model, we won't always have access to a `\"cuda\"` (GPU) device.\n",
        "    * Making the predictions on CPU will be a good indicator of speed of inference too because generally predictions on CPU devices are slower than GPU devices.\n",
        "2. **Transforms** - We'll also be sure to set the `transform` parameter to `effnetb2_transforms` to make sure the images are opened and transformed in the same way our `effnetb2` model has been trained on."
      ],
      "metadata": {
        "id": "vJU9TnhnjeOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions across the test dataset with EffNetB2\n",
        "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                    model=effnetb2,\n",
        "                                    transform=effnetb2_transforms,\n",
        "                                    class_names=class_names,\n",
        "                                    device=\"cpu\")"
      ],
      "metadata": {
        "id": "iMbnYv_fjeMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the first 2 prediction dictionaries\n",
        "effnetb2_test_pred_dicts[:2]"
      ],
      "metadata": {
        "id": "ZMhy9nWGjeJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like our `pred_and_store()` function worked nicely.\n",
        "\n",
        "Thanks to our list of dictionaries data structure, we've got plenty of useful information we can further inspect.\n",
        "\n",
        "To do so, let's turn our list of dictionaries into a pandas DataFrame."
      ],
      "metadata": {
        "id": "y3_YQ2K4jeHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
        "effnetb2_test_pred_df.head()"
      ],
      "metadata": {
        "id": "1Cy7EpgpjeEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of correct predictions\n",
        "effnetb2_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "hJfWc-gQjeB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average time per predicition\n",
        "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(),4)\n",
        "print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")"
      ],
      "metadata": {
        "id": "mg5dgk0At2zG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4c0f70-1c39-409c-8c8f-b2e95c24260c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EffNetB2 average time per prediction: 0.1002 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "how does that average prediction time live up to our criteria of our model performing at real-time (~30FPS or 0.03 seconds per prediction)?"
      ],
      "metadata": {
        "id": "O_o0jh66t2t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add EffnetB2 average prediction time to stats dictionary\n",
        "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "8ooxJWTbt2rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Making and timing predictions with ViT\n",
        "\n",
        "We've made predictions with our EffNetB2 model, now let's do the same for our ViT model.\n",
        "\n",
        "To do so, we can use the `pred_and_store()` function we created above except this time we'll pass in our `vit` model as well as the `vit_transforms`.\n",
        "\n",
        "And we'll keep the predictions on the CPU via `device=\"cpu\"` (a natural extension here would be to test the prediction times on CPU and on GPU)."
      ],
      "metadata": {
        "id": "E4MGXHNAuu0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a list of prediction dictionaries with ViT feature extractor model on test images\n",
        "vit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                      model=vit,\n",
        "                                      transform=vit_transforms,\n",
        "                                      class_names=class_names,\n",
        "                                      device=\"cpu\")"
      ],
      "metadata": {
        "id": "D0Y4hK_GuuyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first couple of ViT predictions on the test dataset\n",
        "vit_test_pred_dicts[:2]"
      ],
      "metadata": {
        "id": "9tmD5qZuuuvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d240ccba-4e13-4473-cb14-75d1f83cc556"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/670345.jpg'),\n",
              "  'class_name': 'steak',\n",
              "  'pred_prob': 0.9985,\n",
              "  'pred_class': 'steak',\n",
              "  'time_for_pred': 0.7579,\n",
              "  'correct': True},\n",
              " {'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/2756200.jpg'),\n",
              "  'class_name': 'steak',\n",
              "  'pred_prob': 0.9194,\n",
              "  'pred_class': 'steak',\n",
              "  'time_for_pred': 0.476,\n",
              "  'correct': True}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn vit_test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
        "vit_test_pred_df.head()"
      ],
      "metadata": {
        "id": "kuc2YGjduusL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of correct predictions\n",
        "vit_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "uf9FNWUFuupb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Our ViT model did a little better than our EffNetB2 model in terms of correct predictions, only two samples wrong across the whole test dataset.\n",
        "\n",
        "As an extension we'll visualize the ViT model's wrong predictions and see if there's any reason why it might've got them wrong.\n",
        "\n",
        "How about we calculate how long the ViT model took per prediction?"
      ],
      "metadata": {
        "id": "sTXjbdcFuumr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the average time per prediction\n",
        "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(),4)\n",
        "print(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\")"
      ],
      "metadata": {
        "id": "btylqoWBu901"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add average prediction time for ViT model on CPU\n",
        "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
        "vit_stats"
      ],
      "metadata": {
        "id": "5wmu6VGPu9x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Comparing model results, prediction times and size\n",
        "\n",
        "Our two best model contenders have been trained and evaluated.\n",
        "\n",
        "Now let's put them head to head and compare across their different statistics.\n",
        "\n",
        "To do so, let's turn our `effnetb2_stats` and `vit_stats` dictionaries into a pandas DataFrame.\n",
        "\n",
        "We'll add a column to view the model names as well as convert the test accuracy to a whole percentage rather than decimal."
      ],
      "metadata": {
        "id": "b1NzOLDUu9u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn stat dictionaries into DataFrame\n",
        "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
        "\n",
        "# add column for model names\n",
        "df[\"model\"] = [\"EffNetB2\",\"ViT\"]\n",
        "\n",
        "# convert accuracy to percentages\n",
        "df[\"test_acc\"] = round(df[\"test_acc\"]*100 ,2 )\n",
        "df"
      ],
      "metadata": {
        "id": "RszGAharu9sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems our models are quite close in terms of overall test accuracy but how do they look across the other fields?\n",
        "\n",
        "One way to find out would be to divide the ViT model statistics by the EffNetB2 model statistics to find out the different ratios between the models.\n",
        "\n",
        "Let's create another DataFrame to do so."
      ],
      "metadata": {
        "id": "GpiSNwaEu9pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare ViT to EffNetB2 across different characteristics\n",
        "pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics\n",
        "             columns=[\"ViT to EffNetB2 ratios\"]).T"
      ],
      "metadata": {
        "id": "LYVl-TO8u9ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems our ViT model outperforms the EffNetB2 model across the performance metrics (test loss, where lower is better and test accuracy, where higher is better) but at the expense of having:\n",
        "* More number of parameters.\n",
        "* Larger model size.\n",
        "* More prediction time per image.\n",
        "\n",
        "Are these tradeoffs worth it?\n",
        "\n",
        "Perhaps if we had unlimited compute power but for our use case of deploying the FoodVision Mini model to a smaller device (e.g. a mobile phone), we'd likely start out with the EffNetB2 model for faster predictions at a slightly reduced performance but dramatically smaller size."
      ],
      "metadata": {
        "id": "UdJb0pzBu9jt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Visualizing the speed vs. performance tradeoff\n",
        "\n",
        "We've seen that our ViT model outperforms our EffNetB2 model in terms of performance metrics such as test loss and test accuracy.\n",
        "\n",
        "However, our EffNetB2 model performs predictions faster and has a much smaller model size.\n",
        "\n",
        "> **Note:** Performance or inference time is also often referred to as \"latency\".\n",
        "\n",
        "How about we make this fact visual?\n",
        "\n",
        "We can do so by creating a plot with matplotlib:\n",
        "1. Create a scatter plot from the comparison DataFrame to compare EffNetB2 and ViT `time_per_pred_cpu` and `test_acc` values.\n",
        "2. Add titles and labels respective of the data and customize the fontsize for aesthetics.\n",
        "3. Annotate the samples on the scatter plot from step 1 with their appropriate labels (the model names).\n",
        "4. Create a legend based on the model sizes (`model_size (MB)`)."
      ],
      "metadata": {
        "id": "Vs1YJODhu9gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a plot from model comparison DatFrame\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "scatter = ax.scatter(data=df,\n",
        "                     x=\"time_per_pred_cpu\",\n",
        "                     y=\"test_acc\",\n",
        "                     c=[\"blue\",\"orange\"],\n",
        "                     s=\"model_size (MB)\")\n",
        "\n",
        "# 2. Add titles, labels and customize fontsize for aesthetics\n",
        "ax.set_title(\"FoodVision Mini Inference speed vs performance\",fontsize=18)\n",
        "ax.set_xlabel(\"Prediction time per image (seconds)\",fontsize=14)\n",
        "ax.set_ylabel(\"Test accuracy (%)\",fontsize=14)\n",
        "ax.tick_params(axis=\"both\",labelsize=14)\n",
        "ax.grid(True)\n",
        "\n",
        "# 3. Annotate the scatter plot with the model names\n",
        "for index,row in df.iterrows():\n",
        "    ax.annotate(text=row[\"model\"],\n",
        "                xy=(row[\"time_per_pred_cpu\"]+0.0006,row[\"test_acc\"]+0.03),size=12)\n",
        "\n",
        "# 4. Create a legend based on model sizes\n",
        "handles, labels = scatter.legend_elements(prop=\"sizes\",alpha=0.5)\n",
        "model_size_legend = ax.legend(handles,\n",
        "                              labels,\n",
        "                              loc=\"lower right\",\n",
        "                              title=\"Model size (MB)\",\n",
        "                              fontsize=12)\n",
        "\n",
        "# save the figure\n",
        "!mkdir images/\n",
        "plt.savefig(\"images/09_foodvision_mini_inference_speed_vs_performance.png\")\n",
        "\n",
        "# show the figure\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "95aJGCvzu9d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot visualizes the **speed vs. performance tradeoff**, in other words, when we have a larger, better performing deep model (like our ViT model), it *generally* takes longer to perform inference (higher latency).\n",
        "And it can be tempting to just deploy the *best* performing model but it's also good to take into consideration where the model is going to be performing.\n",
        "\n",
        "In our case, the differences between our model's performance levels (on the test loss and test accuracy) aren't too extreme.\n",
        "\n",
        "But since we'd like to put an emphasis on speed to begin with, we're going to stick with deploying EffNetB2 since it's faster and has a much smaller footprint.\n"
      ],
      "metadata": {
        "id": "1Kptcb83u9au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Bringing FoodVision Mini to life by creating a Gradio demo\n",
        "\n",
        "We've decided we'd like to deploy the EffNetB2 model (to begin with, this could always be changed later).\n",
        "\n",
        "So how can we do that?\n",
        "\n",
        "There are several ways to deploy a machine learning model each with specific use cases (as discussed above).\n",
        "\n",
        "We're going to be focused on perhaps the quickest and certainly one of the most fun ways to get a model deployed to the internet.\n"
      ],
      "metadata": {
        "id": "I6bB3e9hu9YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import/install gradio\n",
        "try:\n",
        "  import gradio as gr\n",
        "except:\n",
        "  !pip install gradio\n",
        "  import gradio as gr\n",
        "\n",
        "print(f\"gradio version: {gr.__version__}\")"
      ],
      "metadata": {
        "id": "WhMM7fvUu9VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Gradio overview\n",
        "\n",
        "The overall premise of Gradio is very similar to what we've been repeating throughout the course.\n",
        "\n",
        "What are our **inputs** and **outputs**?\n",
        "\n",
        "And how should we get there?\n",
        "\n",
        "Well that's what our machine learning model does.\n",
        "\n",
        "```\n",
        "inputs -> ML model -> outputs\n",
        "```\n",
        "\n",
        "In our case, for FoodVision Mini, our inputs are images of food, our ML model is EffNetB2 and our outputs are classes of food (pizza, steak or sushi).\n",
        "\n",
        "```\n",
        "images of food -> EffNetB2 -> outputs\n",
        "```\n",
        "\n",
        "Though the concepts of inputs and outputs can be bridged to almost any other kind of ML problem.\n",
        "\n",
        "Our inputs and outputs might be any combination of the following:\n",
        "* Images\n",
        "* Text\n",
        "* Video\n",
        "* Tabular data\n",
        "* Audio\n",
        "* Numbers\n",
        "* & more\n",
        "\n",
        "And the ML model you build will depend on your inputs and outputs.\n",
        "\n",
        "Gradio emulates this paradigm by creating an interface ([`gradio.Interface()`](https://gradio.app/docs/#interface-header)) from inputs to outputs.\n",
        "\n",
        "```\n",
        "gradio.Interface(fn, inputs, outputs)\n",
        "```\n",
        "\n",
        "Where, `fn` is a Python function to map the `inputs` to the `outputs`.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-gradio-workflow.png\" alt=\"gradio workflow of inputs flowing into some kind of model or function and then producing outputs\" width=900/>\n",
        "\n",
        "*Gradio provides a very helpful `Interface` class to easily create an inputs -> model/function -> outputs workflow where the inputs and outputs could be almost anything you want. For example, you might input Tweets (text) to see if they're about machine learning or not or [input a text prompt to generate images](https://huggingface.co/blog/stable_diffusion).*\n",
        "\n",
        "> **Note:** Gradio has a vast number of possible `inputs` and `outputs` options known as \"Components\" from images to text to numbers to audio to videos and more. We can see all of these in the [Gradio Components documentation](https://gradio.app/docs/#components)."
      ],
      "metadata": {
        "id": "KUOljQl1u9SO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Creating a function to map our inputs and outputs\n",
        "\n",
        "To create our FoodVision Mini demo with Gradio, we'll need a function to map our inputs to our outputs.\n",
        "\n",
        "We created a function earlier called `pred_and_store()` to make predictions with a given model across a list of target files and store them in a list of dictionaries.\n",
        "\n",
        "How about we create a similar function but this time focusing on making a prediction on a single image with our EffNetB2 model?\n",
        "\n",
        "More specifically, we want a function that takes an image as input, preprocesses (transforms) it, makes a prediction with EffNetB2 and then returns the prediction (pred or pred label for short) as well as the prediction probability (pred prob).\n",
        "\n",
        "And while we're here, let's return the time it took to do so too:\n",
        "\n",
        "```\n",
        "input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time taken\n",
        "```\n",
        "\n",
        "This will be our `fn` parameter for our Gradio interface.\n",
        "\n",
        "First, let's make sure our EffNetB2 model is on the CPU (since we're sticking with CPU-only predictions, however you could change this if you have access to a GPU)."
      ],
      "metadata": {
        "id": "pmsot8LIyLih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put EffNetb2 on cpu\n",
        "effnetb2.to(\"cpu\")\n",
        "\n",
        "next(iter(effnetb2.parameters())).device"
      ],
      "metadata": {
        "id": "TqapNp1FyLfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's create a function called `predict()` to replicate the workflow above."
      ],
      "metadata": {
        "id": "BGURfB3ay-qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple , Dict\n",
        "\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "    \"\"\"\n",
        "    # start the timer\n",
        "    start_time = timer()\n",
        "\n",
        "    # transform the target image and add a batch dimension\n",
        "    img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "    # put model into evaluation mode and turn on inference mode\n",
        "    effnetb2.eval()\n",
        "    with torch.inference_mode():\n",
        "        # pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
        "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "    # create a prediction label and prediction probability dictionary for each prediction class (this is the required format for gradio's out parameter)\n",
        "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "    # calculate the prediction time\n",
        "    pred_time = round(timer()-start_time,5)\n",
        "\n",
        "    # return the prediction dictionary and prediction time in seconds\n",
        "    return pred_labels_and_probs, pred_time\n"
      ],
      "metadata": {
        "id": "9kxJJzz7y-oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beautiful!\n",
        "\n",
        "Now let's see our function in action by performing a prediction on a random image from the test dataset.\n",
        "\n",
        "We'll start by getting a list of all the image paths from the test directory and then randomly selecting one.\n",
        "\n",
        "Then we'll open the randomly selected image with [`PIL.Image.open()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#functions).\n",
        "\n",
        "Finally, we'll pass the image to our `predict()` function."
      ],
      "metadata": {
        "id": "xuzOOpDty-mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# get a list of all test image paths\n",
        "test_image_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "\n",
        "# select a random image path\n",
        "random_image_path = random.sample(test_data_paths,k=1)[0]\n",
        "\n",
        "# open the target image\n",
        "image = Image.open(random_image_path)\n",
        "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
        "\n",
        "# make a prediction on the image and get the prediction class and prediction probability\n",
        "pred_dict, pred_time = predict(img=image)\n",
        "print(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\n",
        "print(f\"Prediction time: {pred_time} seconds\")"
      ],
      "metadata": {
        "id": "2etz0kM2y-iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Creating a list of example images\n",
        "\n",
        "Our `predict()` function enables us to go from inputs -> transform -> ML model -> outputs.\n",
        "\n",
        "Which is exactly what we need for our Graido demo.\n",
        "\n",
        "But before we create the demo, let's create one more thing: a list of examples.\n",
        "\n",
        "Gradio's [`Interface`](https://gradio.app/docs/#interface) class takes a list of `examples` of as an optional parameter (`gradio.Interface(examples=List[Any])`).\n",
        "\n",
        "And the format for the `examples` parameter is a list of lists.\n",
        "\n",
        "So let's create a list of lists containing random filepaths to our test images.\n",
        "\n",
        "Three examples should be enough."
      ],
      "metadata": {
        "id": "Ea1W52wLy-gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of example inputs to our gradio demo\n",
        "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths,k=3)]\n",
        "example_list"
      ],
      "metadata": {
        "id": "fc__IR2Jy-db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our Gradio demo will showcase these as example inputs to our demo so people can try it out and see what it does without uploading any of their own data."
      ],
      "metadata": {
        "id": "nDZyt-l-y-a8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 Building a Gradio interface\n",
        "\n",
        "Time to put everything together and bring our FoodVision Mini demo to life!\n",
        "\n",
        "Let's create a Gradio interface to replicate the workflow:\n",
        "\n",
        "```\n",
        "input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time taken\n",
        "```\n",
        "\n",
        "We can do with the [`gradio.Interface()`](https://gradio.app/docs/#interface) class with the following parameters:\n",
        "* `fn` - a Python function to map `inputs` to `outputs`, in our case, we'll use our `predict()` function.\n",
        "* `inputs` - the input to our interface, such as an image using [`gradio.Image()`](https://gradio.app/docs/#image) or `\"image\"`.\n",
        "* `outputs` - the output of our interface once the `inputs` have gone through the `fn`, such as a label using [`gradio.Label()`](https://gradio.app/docs/#label) (for our model's predicted labels) or number using [`gradio.Number()`](https://gradio.app/docs/#number) (for our model's prediction time).\n",
        "    * **Note:** Gradio comes with many in-built `inputs` and `outputs` options known as [\"Components\"](https://gradio.app/docs/#components).\n",
        "* `examples` - a list of examples to showcase for the demo.\n",
        "* `title` - a string title of the demo.\n",
        "* `description` - a string description of the demo.\n",
        "* `article` - a reference note at the bottom of the demo.\n",
        "\n",
        "Once we've created our demo instance of `gr.Interface()`, we can bring it to life using [`gradio.Interface().launch()`](https://gradio.app/docs/#launch-header) or `demo.launch()` command.\n"
      ],
      "metadata": {
        "id": "4C-zXy09y-Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "# create title, description and article strings\n",
        "title = \"FoodVision Mini 🍕🥩🍣\"\n",
        "description = \"An EfficientNetB2 feature extractor computer vision model to classify food images into 3 classes: pizza, steak and sushi.\"\n",
        "article = \"Created at [09. Model Deployment Project](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
        "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths,k=3)]\n",
        "# create the gradio demo\n",
        "demo = gr.Interface(fn=predict,\n",
        "                    inputs=gr.Image(type=\"pil\"),\n",
        "                    outputs=[gr.Label(num_top_classes=3,\n",
        "                    label=\"Predictions\"),\n",
        "                             gr.Number(label=\"Prediction time (s)\")],\n",
        "\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# launch the demo\n",
        "demo.launch(debug=False, # print errors locally?\n",
        "            share=True, # generate a publically shareable url\n",
        "            )"
      ],
      "metadata": {
        "id": "h8gkYND9y-WC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "7042f714-4e6c-40f2-c41a-0da6f0e1fd6e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a85e643169636c11ce.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a85e643169636c11ce.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Turning our FoodVision Mini Gradio Demo into a deployable app\n",
        "\n",
        "We've seen our FoodVision Mini model come to life through a Gradio demo.\n",
        "\n",
        "But what if we wanted to share it with our friends?\n",
        "\n",
        "Well, we could use the provided Gradio link, however, the shared link only lasts for 72-hours.\n",
        "\n",
        "To make our FoodVision Mini demo more permanent, we can package it into an app and upload it to [Hugging Face Spaces](https://huggingface.co/spaces/launch)."
      ],
      "metadata": {
        "id": "60_kZhRh1tUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 What is Hugging Face Spaces?\n",
        "\n",
        "Hugging Face Spaces is a resource that allows you to host and share machine learning apps.\n",
        "\n",
        "Building a demo is one of the best ways to showcase and test what you've done.\n",
        "\n",
        "And Spaces allows you to do just that.\n",
        "\n",
        "You can think of Hugging Face as the GitHub of machine learning.\n",
        "\n",
        "If having a good GitHub portfolio showcases your coding abilities, having a good Hugging Face portfolio can showcase your machine learning abilities.\n",
        "\n",
        "> **Note:** There are many other places we could upload and host our Gradio app such as, Google Cloud, AWS (Amazon Web Services) or other cloud vendors, however, we're going to use Hugging Face Spaces due to the ease of use and wide adoption by the machine learning community."
      ],
      "metadata": {
        "id": "6usrFTtO1tSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Deployed Gradio app structure\n",
        "\n",
        "To upload our demo Gradio app, we'll want to put everything relating to it into a single directory.\n",
        "\n",
        "For example, our demo might live at the path `demos/foodvision_mini/` with the file structure:\n",
        "\n",
        "```\n",
        "demos/\n",
        "└── foodvision_mini/\n",
        "    ├── 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
        "    ├── app.py\n",
        "    ├── examples/\n",
        "    │   ├── example_1.jpg\n",
        "    │   ├── example_2.jpg\n",
        "    │   └── example_3.jpg\n",
        "    ├── model.py\n",
        "    └── requirements.txt\n",
        "```\n",
        "\n",
        "Where:\n",
        "* `09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth` is our trained PyTorch model file.\n",
        "* `app.py` contains our Gradio app (similar to the code that launched the app).\n",
        "    * **Note:** `app.py` is the default filename used for Hugging Face Spaces, if you deploy your app there, Spaces will by default look for a file called `app.py` to run. This is changeable in settings.\n",
        "* `examples/` contains example images to use with our Gradio app.\n",
        "* `model.py` contains the model definition as well as any transforms associated with the model.\n",
        "* `requirements.txt` contains the dependencies to run our app such as `torch`, `torchvision` and `gradio`.\n",
        "\n",
        "Why this way?\n",
        "\n",
        "Because it's one of the simplest layouts we could begin with.\n",
        "\n",
        "Our focus is: *experiment, experiment, experiment!*\n",
        "\n",
        "The quicker we can run smaller experiments, the better our bigger ones will be.\n",
        "\n",
        "We're going to work towards recreating the structure above but you can see a live demo app running on Hugging Face Spaces as well as the file structure:\n",
        "* [Live Gradio demo of FoodVision Mini 🍕🥩🍣](https://huggingface.co/spaces/mrdbourke/foodvision_mini).\n",
        "* [FoodVision Mini file structure on Hugging Face Spaces](https://huggingface.co/spaces/mrdbourke/foodvision_mini/tree/main)."
      ],
      "metadata": {
        "id": "dPL-XiDu1tOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Creating a `demos` folder to store our FoodVision Mini app files\n",
        "\n",
        "To begin, let's first create a `demos/` directory to store all of our FoodVision Mini app files.\n",
        "\n",
        "We can do with Python's [`pathlib.Path(\"path_to_dir\")`](https://docs.python.org/3/library/pathlib.html#basic-use) to establish the directory path and [`pathlib.Path(\"path_to_dir\").mkdir()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir) to create it."
      ],
      "metadata": {
        "id": "gJpOYDdX1tME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# create foodvision mini demo path\n",
        "foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n",
        "\n",
        "# if the path exists, remove it\n",
        "if foodvision_mini_demo_path.exists():\n",
        "  print(f\"{foodvision_mini_demo_path} directory exists... removing directory\")\n",
        "  shutil.rmtree(foodvision_mini_demo_path)\n",
        "\n",
        "# if the file doesn't exist, create it\n",
        "if not foodvision_mini_demo_path.exists():\n",
        "  print(f\"Creating {foodvision_mini_demo_path} directory\")\n",
        "  foodvision_mini_demo_path.mkdir(parents=True,\n",
        "                                  exist_ok=True)\n",
        "\n",
        "# check what's in the folder\n",
        "!ls {foodvision_mini_demo_path}"
      ],
      "metadata": {
        "id": "LbhCDtki1tJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4 Creating a folder of example images to use with our FoodVision Mini demo\n",
        "\n",
        "Now we've got a directory to store our FoodVision Mini demo files, let's add some examples to it.\n",
        "\n",
        "Three example images from the test dataset should be enough.\n",
        "\n",
        "To do so we'll:\n",
        "1. Create an `examples/` directory within the `demos/foodvision_mini` directory.\n",
        "2. Choose three random images from the test dataset and collect their filepaths in a list.\n",
        "3. Copy the three random images from the test dataset to the `demos/foodvision_mini/examples/` directory."
      ],
      "metadata": {
        "id": "U7OYDme-1tHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Create an example directory\n",
        "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples/\"\n",
        "foodvision_mini_examples_path.mkdir(parents=True,\n",
        "                                    exist_ok=True)\n",
        "\n",
        "# 2. Collect three random test dataset image paths\n",
        "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
        "\n",
        "# 3. Copy the three random images to example directory\n",
        "for example in foodvision_mini_examples:\n",
        "  destination = foodvision_mini_examples_path / example.name\n",
        "  print(f\"Copying {example} to {destination}\")\n",
        "  shutil.copy2(src=example,\n",
        "               dst=destination)"
      ],
      "metadata": {
        "id": "tKyRYVbg1tFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to verify our examples are present, let's list the contents of our `demos/foodvision_mini/examples/` directory with [`os.listdir()`](https://docs.python.org/3/library/os.html#os.listdir) and then format the filepaths into a list of lists (so it's compatible with Gradio's [`gradio.Interface()`](https://gradio.app/docs/#interface) `example` parameter)."
      ],
      "metadata": {
        "id": "RKRuOzLB1tC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# get examples of filepaths in a list of lists\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
        "example_list"
      ],
      "metadata": {
        "id": "vviolz2g1tAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa84a4ad-bdaf-4d72-cb1d-d9456173f184"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['examples/3622237.jpg'], ['examples/2582289.jpg'], ['examples/592799.jpg']]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b24ju-CRhBd",
        "outputId": "3012555a-706e-4d6d-b796-8b57fb10c1f9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['examples/3622237.jpg'], ['examples/2582289.jpg'], ['examples/592799.jpg']]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5 Moving our trained EffNetB2 model to our FoodVision Mini demo directory\n",
        "\n",
        "We previously saved our FoodVision Mini EffNetB2 feature extractor model under `models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth`.\n",
        "\n",
        "And rather double up on saved model files, let's move our model to our `demos/foodvision_mini` directory.\n",
        "\n",
        "We can do so using Python's [`shutil.move()`](https://docs.python.org/3/library/shutil.html#shutil.move) method and passing in `src` (the source path of the target file) and `dst` (the destination path of the target file to be moved to) parameters."
      ],
      "metadata": {
        "id": "yLm3mbP54JYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create a source path for our target model\n",
        "effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
        "\n",
        "# Create a destination path for our target model\n",
        "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
        "\n",
        "# Try to move the file\n",
        "try:\n",
        "    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
        "\n",
        "    # Move the model\n",
        "    shutil.move(src=effnetb2_foodvision_mini_model_path,\n",
        "                dst=effnetb2_foodvision_mini_model_destination)\n",
        "\n",
        "    print(f\"[INFO] Model move complete.\")\n",
        "\n",
        "# If the model has already been moved, check if it exists\n",
        "except:\n",
        "    print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n",
        "    print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")"
      ],
      "metadata": {
        "id": "Nz9J7oKC4JWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34cc00be-5483-4848-8287-8d1f44edde5b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Attempting to move models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth to demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
            "[INFO] Model move complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6 Turning our EffNetB2 model into a Python script (`model.py`)\n",
        "\n",
        "Our current model's `state_dict` is saved to `demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth`.\n",
        "\n",
        "To load it in we can use `model.load_state_dict()` along with `torch.load()`.\n",
        "\n",
        "> **Note:** For a refresh on saving and loading a model (or a model's `state_dict` in PyTorch, see [01. PyTorch Workflow Fundamentals section 5: Saving and loading a PyTorch model](https://www.learnpytorch.io/01_pytorch_workflow/#5-saving-and-loading-a-pytorch-model) or see the PyTorch recipe for [What is a `state_dict` in PyTorch?](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html)\n",
        "\n",
        "But before we can do this, we first need a way to instantiate a `model`.\n",
        "\n",
        "To do this in a modular fashion we'll create a script called `model.py` which contains our `create_effnetb2_model()` function we created in [section 3.1: *Creating a function to make an EffNetB2 feature extractor*](https://www.learnpytorch.io/09_pytorch_model_deployment/#31-creating-a-function-to-make-an-effnetb2-feature-extractor).\n",
        "\n",
        "That way we can import the function in *another* script (see `app.py` below) and then use it to create our EffNetB2 `model` instance as well as get its appropriate transforms.\n",
        "\n",
        "Just like in [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/), we'll use the `%%writefile path/to/file` magic command to turn a cell of code into a file."
      ],
      "metadata": {
        "id": "CNGQHIcq4JTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "def create_effnetb2_model(num_classes: int=3,\n",
        "                          seed:int=42):\n",
        "    # create a effnetb2 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # freeze the base layers in the model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # change the classifier head\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408,\n",
        "                  out_features=num_classes)\n",
        "    )\n",
        "    return model, transforms\n"
      ],
      "metadata": {
        "id": "ybBS-VGj4JQS"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.7 Turning our FoodVision Mini Gradio app into a Python script (`app.py`)\n",
        "\n",
        "We've now got a `model.py` script as well as a path to a saved model `state_dict` that we can load in.\n",
        "\n",
        "Time to construct `app.py`.\n",
        "\n",
        "We call it `app.py` because by default when you create a HuggingFace Space, it looks for a file called `app.py` to run and host (though you can change this in settings).\n",
        "\n",
        "Our `app.py` script will put together all of the pieces of the puzzle to create our Gradio demo and will have four main parts:\n",
        "\n",
        "1. **Imports and class names setup** - Here we'll import the various dependencies for our demo including the `create_effnetb2_model()` function from `model.py` as well as setup the different class names for our FoodVision Mini app.\n",
        "2. **Model and transforms preparation** - Here we'll create an EffNetB2 model instance along with the transforms to go with it and then we'll load in the saved model weights/`state_dict`. When we load the model we'll also set `map_location=torch.device(\"cpu\")` in [`torch.load()`](https://pytorch.org/docs/stable/generated/torch.load.html) so our model gets loaded onto the CPU regardless of the device it trained on (we do this because we won't necessarily have a GPU when we deploy and we'll get an error if our model is trained on GPU but we try to deploy it to CPU without explicitly saying so).\n",
        "3. **Predict function** - Gradio's `gradio.Interface()` takes a `fn` parameter to map inputs to outputs, our `predict()` function will be the same as the one we defined above in [section 7.2: *Creating a function to map our inputs and outputs*](https://www.learnpytorch.io/09_pytorch_model_deployment/#72-creating-a-function-to-map-our-inputs-and-outputs), it will take in an image and then use the loaded transforms to preprocess it before using the loaded model to make a prediction on it.\n",
        "    * **Note:** We'll have to create the example list on the fly via the `examples` parameter. We can do so by creating a list of the files inside the `examples/` directory with: `[[\"examples/\" + example] for example in os.listdir(\"examples\")]`.\n",
        "4. **Gradio app** - This is where the main logic of our demo will live, we'll create a `gradio.Interface()` instance called `demo` to put together our inputs, `predict()` function and outputs. And we'll finish the script by calling `demo.launch()` to launch our FoodVision Mini demo!"
      ],
      "metadata": {
        "id": "WsTZQKMn4JNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports and class names setups\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# setup class names\n",
        "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
        "\n",
        "# 2. Model and transforms preparation\n",
        "# create model\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
        "    num_classes=3\n",
        ")\n",
        "\n",
        "# load saved weights\n",
        "effnetb2.load_state_dict(\n",
        "    torch.load(\n",
        "        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n",
        "        map_location=torch.device(\"cpu\"),\n",
        "        )\n",
        ")\n",
        "\n",
        "# 3. Predict function\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\"\"\"\n",
        "\n",
        "    start_time = timer()\n",
        "    img=effnetb2_transforms(img).unsqueeze(0)\n",
        "    effnetb2.eval()\n",
        "    with torch.inference_mode():\n",
        "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "    pred_time = round(timer()-start_time,5)\n",
        "    return pred_labels_and_probs, pred_time\n",
        "\n",
        "\n",
        "# 4. Gradio App\n",
        "title=\"FoodVision Mini 🍕🥩🍣\"\n",
        "description=\"An EfficientNetB2 feature extractor computer vision model to classify food images into 3 classes: pizza, steak and sushi.\"\n",
        "article=\"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
        "\n",
        "# create the gradio demo\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=[\n",
        "        gr.Label(num_top_classes=3, label=\"Predictions\"),\n",
        "        gr.Number(label=\"Prediction time (s)\")\n",
        "    ],\n",
        "\n",
        "    title=title,\n",
        "    description=description,\n",
        "    article=article\n",
        ")\n",
        "\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "sNA5HPay6XWx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "7a5a99e0-c49f-43b5-c705-abebeb69cdcf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-f92a50915726>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_effnetb2_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimeit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_timer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.8 Creating a requirements file for FoodVision Mini (`requirements.txt`)"
      ],
      "metadata": {
        "id": "v6j1qLCg6XPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/requirements.txt\n",
        "httpx==0.24.1\n",
        "httpcore==0.17.0\n",
        "gradio\n",
        "huggingface_hub"
      ],
      "metadata": {
        "id": "NwxAZZW36XJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ngJ_ECaNjJB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Deploying our FoodVision Mini app to HuggingFace Spaces\n",
        "\n",
        "We've got a file containing our FoodVision Mini demo, now how do we get it to run on Hugging Face Spaces?\n",
        "\n",
        "There are two main options for uploading to a Hugging Face Space (also called a [Hugging Face Repository](https://huggingface.co/docs/hub/repositories-getting-started#getting-started-with-repositories), similar to a git repository):\n",
        "1. [Uploading via the Hugging Face Web interface (easiest)](https://huggingface.co/docs/hub/repositories-getting-started#adding-files-to-a-repository-web-ui).\n",
        "2. [Uploading via the command line or terminal](https://huggingface.co/docs/hub/repositories-getting-started#terminal).\n",
        "    * **Bonus:** We can also use the [`huggingface_hub` library](https://huggingface.co/docs/huggingface_hub/index) to interact with Hugging Face, this would be a good extension to the above two options.\n",
        "\n",
        "Feel free to read the documentation on both options but we're going to go with option two.\n",
        "\n",
        "> **Note:** To host anything on Hugging Face, you will need to [sign up for a free Hugging Face account](https://huggingface.co/join)."
      ],
      "metadata": {
        "id": "Se0Sa8Bk6XEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1 Downloading our FoodVision Mini app files\n",
        "\n",
        "Let's check out the demo files we've got inside `demos/foodvision_mini`.\n",
        "\n",
        "To do so we can use the `!ls` command followed by the target filepath.\n",
        "\n",
        "`ls` stands for \"list\" and the `!` means we want to execute the command at the shell level."
      ],
      "metadata": {
        "id": "vMncVUDv7bF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls demos/foodvision_mini/"
      ],
      "metadata": {
        "id": "2QCgj6aX7bDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are all files that we've created!\n",
        "\n",
        "To begin uploading our files to Hugging Face, let's now download them from Google Colab (or wherever you're running this notebook).\n",
        "\n",
        "To do so, we'll first compress the files into a single zip folder via the command:\n",
        "\n",
        "```\n",
        "zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
        "```\n",
        "\n",
        "Where:\n",
        "* `zip` stands for \"zip\" as in \"please zip together the files in the following directory\".\n",
        "* `-r` stands for \"recursive\" as in, \"go through all of the files in the target directory\".\n",
        "* `../foodvision_mini.zip` is the target directory we'd like our files to be zipped to.\n",
        "* `*` stands for \"all the files in the current directory\".\n",
        "* `-x` stands for \"exclude these files\".\n",
        "\n",
        "We can download our zip file from Google Colab using [`google.colab.files.download(\"demos/foodvision_mini.zip\")`](https://colab.research.google.com/notebooks/io.ipynb) (we'll put this inside a `try` and `except` block just in case we're not running the code inside Google Colab, and if so we'll print a message saying to manually download the files).\n",
        "\n",
        "Let's try it out!"
      ],
      "metadata": {
        "id": "mNVpi9Ow7bA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change into and the zip the foodvision_mini folder but exclude certain files\n",
        "!cd demos/foodvision_mini && zip -r ../../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
        "\n",
        "# download the zipped FoodVision Mini app (if running in Google Colab)\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download(\"demos/foodvision_mini.zip\")\n",
        "except:\n",
        "  print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")"
      ],
      "metadata": {
        "id": "urzzD2KG7a6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 Running our FoodVision Mini demo locally\n",
        "\n",
        "If you download the `foodvision_mini.zip` file, you can test it locally by:\n",
        "1. Unzipping the file.\n",
        "2. Opening terminal or a command line prompt.\n",
        "3. Changing into the `foodvision_mini` directory (`cd foodvision_mini`).\n",
        "4. Creating an environment (`python3 -m venv env`).\n",
        "5. Activating the environment (`source env/bin/activate`).\n",
        "5. Installing the requirements (`pip install -r requirements.txt`, the \"`-r`\" is for recursive).\n",
        "    * **Note:** This step may take 5-10 minutes depending on your internet connection. And if you're facing errors, you may need to upgrade `pip` first: `pip install --upgrade pip`.\n",
        "6. Run the app (`python3 app.py`).\n",
        "\n",
        "This should result in a Gradio demo just like the one we built above running locally on your machine at a URL such as `http://127.0.0.1:7860/`.\n",
        "\n",
        "> **Note:** If you run the app locally and you notice a `flagged/` directory appear, it contains samples that have been \"flagged\".\n",
        ">\n",
        "> For example, if someone tries the demo and the model produces an incorrect result, the sample can be \"flagged\" and reviewed for later.\n",
        ">\n",
        "> For more on flagging in Gradio, see the [flagging documentation](https://gradio.app/docs/#flagging)."
      ],
      "metadata": {
        "id": "W7x09CEE9VU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Creating FoodVision Big\n",
        "\n",
        "We've spent the past few sections and chapters working on bringing FoodVision Mini to life.\n",
        "\n",
        "And now we've seen it working in a live demo, how about we step things up a notch?\n",
        "\n",
        "How?\n",
        "\n",
        "FoodVision Big!\n",
        "\n",
        "Since FoodVision Mini is trained on pizza, steak and sushi images from the [Food101 dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html) (101 classes of food x 1000 images each), how about we make FoodVision Big by training a model on all 101 classes!\n",
        "\n",
        "We'll go from three classes to 101!\n",
        "\n",
        "From pizza, steak, sushi to pizza, steak, sushi, hot dog, apple pie, carrot cake, chocolate cake, french fries, garlic bread, ramen, nachos, tacos and more!\n",
        "\n",
        "How?\n",
        "\n",
        "Well, we've got all the steps in place, all we have to do is alter our EffNetB2 model slightly as well as prepare a different dataset.\n",
        "\n",
        "To finish Milestone Project 3, let's recreate a Gradio demo similar to FoodVision Mini (three classes) but for FoodVision Big (101 classes).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-foodvision-mini-to-foodvision-big.png\" alt=\"foodvision mini model on three classes: pizza, steak, sushi and foodvision big on all of the 101 classes in the food101 dataset\" width=900/>\n",
        "\n",
        "*FoodVision Mini works with three food classes: pizza, steak and sushi. And FoodVision Big steps it up a notch to work across 101 food classes: all of the [classes in the Food101 dataset](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).*"
      ],
      "metadata": {
        "id": "HZMW1jQJTEnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.1 Creating a model and transforms for FoodVision Big\n",
        "\n",
        "When creating FoodVision Mini we saw that the EffNetB2 model was a good tradeoff between speed and performance (it performed well with a fast speed).\n",
        "\n",
        "So we'll continue using the same model for FoodVision Big.\n",
        "\n",
        "We can create an EffNetB2 feature extractor for Food101 by using our `create_effnetb2_model()` function we created above, in [section 3.1](https://www.learnpytorch.io/09_pytorch_model_deployment/#31-creating-a-function-to-make-an-effnetb2-feature-extractor), and passing it the parameter `num_classes=101` (since Food101 has 101 classes)."
      ],
      "metadata": {
        "id": "rpWF1JmzTNrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a EffNetB2 model capable of fitting to 101 classes for Food101\n",
        "effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)"
      ],
      "metadata": {
        "id": "_4EraazfTP2B"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Get a summary of EffNetB2 feature extractor for Food101 with 101 output classes\n",
        "summary(effnetb2_food101,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40FMxs6XTY9K",
        "outputId": "32dc376c-a6e3-4792-9213-c63c6f39468c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 101]             --                   Partial\n",
              "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
              "│    │    └─SiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
              "│    └─Sequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
              "│    │    └─MBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
              "│    └─Sequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    │    └─MBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    └─Sequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
              "│    │    └─MBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    │    └─MBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    └─Sequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
              "│    │    └─MBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    └─Sequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
              "│    │    └─MBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    └─Sequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
              "│    │    └─MBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    └─Sequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
              "│    │    └─MBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
              "│    └─Conv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
              "│    │    └─SiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
              "├─Sequential (classifier)                                    [1, 1408]            [1, 101]             --                   True\n",
              "│    └─Dropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
              "│    └─Linear (1)                                            [1, 1408]            [1, 101]             142,309              True\n",
              "============================================================================================================================================\n",
              "Total params: 7,843,303\n",
              "Trainable params: 142,309\n",
              "Non-trainable params: 7,700,994\n",
              "Total mult-adds (Units.MEGABYTES): 657.78\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 156.80\n",
              "Params size (MB): 31.37\n",
              "Estimated Total Size (MB): 188.77\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                                                                        \n",
        "See how just like our EffNetB2 model for FoodVision Mini the base layers are frozen (these are pretrained on ImageNet) and the outer layers (the `classifier` layers) are trainable with an output shape of `[batch_size, 101]` (`101` for 101 classes in Food101).\n",
        "\n",
        "Now since we're going to be dealing with a fair bit more data than usual, how about we add a little data augmentation to our transforms (`effnetb2_transforms`) to augment the training data.\n",
        "\n",
        "> **Note:** Data augmentation is a technique used to alter the appearance of an input training sample (e.g. rotating an image or slightly skewing it) to artificially increase the diversity of a training dataset to hopefully prevent overfitting. You can see more on data augmentation in [04. PyTorch Custom Datasets section 6](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation).\n",
        "\n",
        "Let's compose a `torchvision.transforms` pipeline to use [`torchvision.transforms.TrivialAugmentWide()`](https://pytorch.org/vision/main/generated/torchvision.transforms.TrivialAugmentWide.html) (the same data augmentation used by the PyTorch team in their [computer vision recipes](https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#break-down-of-key-accuracy-improvements)) as well as the `effnetb2_transforms` to transform our training images."
      ],
      "metadata": {
        "id": "7mme78x9Tnlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Food101 training data transforms (only perform data augmentation on the training images)\n",
        "food101_train_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.TrivialAugmentWide(),\n",
        "    effnetb2_transforms,\n",
        "])"
      ],
      "metadata": {
        "id": "YLKpGlCVTtD0"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training transforms:\\n{food101_train_transforms}\\n\")\n",
        "print(f\"Testing transforms:\\n{effnetb2_transforms}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKIdNSn7TwaN",
        "outputId": "42e26853-67b0-482e-a41b-cfbfff5029cb"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training transforms:\n",
            "Compose(\n",
            "    TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
            "    ImageClassification(\n",
            "    crop_size=[288]\n",
            "    resize_size=[288]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BICUBIC\n",
            ")\n",
            ")\n",
            "\n",
            "Testing transforms:\n",
            "ImageClassification(\n",
            "    crop_size=[288]\n",
            "    resize_size=[288]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BICUBIC\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2 Getting data for FoodVision Big\n",
        "\n",
        "For FoodVision Mini, we made our own [custom data splits](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) of the entire Food101 dataset.\n",
        "\n",
        "To get the whole Food101 dataset, we can use [`torchvision.datasets.Food101()`](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html).\n",
        "\n",
        "We'll first setup a path to directory `data/` to store the images.\n",
        "\n",
        "Then we'll download and transform the training and testing dataset splits using `food101_train_transforms` and `effnetb2_transforms` to transform each dataset respectively."
      ],
      "metadata": {
        "id": "mLCb-GmkT2iV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "# Setup data directory\n",
        "from pathlib import Path\n",
        "data_dir = Path(\"data\")\n",
        "\n",
        "# Get training data (~750 images x 101 food classes)\n",
        "train_data = datasets.Food101(root=data_dir, # path to download data to\n",
        "                              split=\"train\", # dataset split to get\n",
        "                              transform=food101_train_transforms, # perform data augmentation on training data\n",
        "                              download=True) # want to download?\n",
        "\n",
        "# Get testing data (~250 images x 101 food classes)\n",
        "test_data = datasets.Food101(root=data_dir,\n",
        "                             split=\"test\",\n",
        "                             transform=effnetb2_transforms, # perform normal EffNetB2 transforms on test data\n",
        "                             download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD6ZD_vUT8Lz",
        "outputId": "26747994-2ee6-4649-c1f7-b94760b1ab98"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.00G/5.00G [03:39<00:00, 22.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Food101 class names\n",
        "food101_class_names = train_data.classes\n",
        "\n",
        "# View the first 10\n",
        "food101_class_names[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDozxcb8VOCo",
        "outputId": "99a68aa0-d69a-4f39-8c2c-1e254d278f16"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apple_pie',\n",
              " 'baby_back_ribs',\n",
              " 'baklava',\n",
              " 'beef_carpaccio',\n",
              " 'beef_tartare',\n",
              " 'beet_salad',\n",
              " 'beignets',\n",
              " 'bibimbap',\n",
              " 'bread_pudding',\n",
              " 'breakfast_burrito']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3 Creating a subset of the Food101 dataset for faster experimenting\n",
        "\n",
        "This is optional.\n",
        "\n",
        "We don't *need* to create another subset of the Food101 dataset, we could train and evaluate a model across the whole 101,000 images.\n",
        "\n",
        "But to keep training fast, let's create a 20% split of the training and test datasets.\n",
        "\n",
        "Our goal will be to see if we can beat the original [Food101 paper's](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/) best results with only 20% of the data.\n",
        "\n",
        "To breakdown the datasets we've used/will use:\n",
        "\n",
        "| **Notebook(s)** | **Project name** | **Dataset** | **Number of classes** | **Training images** | **Testing images** |\n",
        "| ----- | ----- | ----- | ----- | ----- | ----- |\n",
        "| 04, 05, 06, 07, 08 | FoodVision Mini (10% data) | Food101 custom split | 3 (pizza, steak, sushi) | 225 | 75 |\n",
        "| 07, 08, 09 | FoodVision Mini (20% data) | Food101 custom split | 3 (pizza, steak, sushi) | 450 | 150 |\n",
        "| **09 (this one)** | FoodVision Big (20% data) | Food101 custom split | 101 (all Food101 classes) | 15150 | 5050 |\n",
        "| Extension | FoodVision Big | Food101 all data | 101 | 75750 | 25250 |\n",
        "\n",
        "Can you see the trend?\n",
        "\n",
        "Just like our model size slowly increased overtime, so has the size of the dataset we've been using for experiments.\n",
        "\n",
        "\n",
        "To make our FoodVision Big (20% data) split, let's create a function called `split_dataset()` to split a given dataset into certain proportions.\n",
        "\n",
        "We can use [`torch.utils.data.random_split()`](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) to create splits of given sizes using the `lengths` parameter.\n",
        "\n",
        "The `lengths` parameter accepts a list of desired split lengths where the total of the list must equal the overall length of the dataset.\n",
        "\n",
        "For example, with a dataset of size 100, you could pass in `lengths=[20, 80]` to receive a 20% and 80% split.\n",
        "\n",
        "We'll want our function to return two splits, one with the target length (e.g. 20% of the training data) and the other with the remaining length (e.g. the remaining 80% of the training data).\n",
        "\n",
        "Finally, we'll set `generator` parameter to a `torch.manual_seed()` value for reproducibility."
      ],
      "metadata": {
        "id": "Db055FuwVl66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset:torchvision.datasets, split_size:float=0.2,seed:int=42):\n",
        "    \"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.\n",
        "\n",
        "    Args:\n",
        "        dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.\n",
        "        split_size (float, optional): How much of the dataset should be split?\n",
        "            E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.\n",
        "        seed (int, optional): Seed for random generator. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and\n",
        "            random_split_2 is of size (1-split_size)*len(dataset).\n",
        "    \"\"\"\n",
        "    # create split lengths based on original dataset length\n",
        "    length_1=int(len(dataset)*split_size)\n",
        "    length_2 = len(dataset) - length_1\n",
        "\n",
        "    # Print out info\n",
        "    print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)\")\n",
        "\n",
        "    # create splits with given random seed\n",
        "    random_split_1, random_split_2 = torch.utils.data.random_split(\n",
        "        dataset=dataset,\n",
        "        lengths=[length_1, length_2],\n",
        "        generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "\n",
        "    return random_split_1, random_split_2"
      ],
      "metadata": {
        "id": "xB6hyQGeVs_a"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training 20% split of Food101\n",
        "train_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n",
        "                                                 split_size=0.2)\n",
        "\n",
        "# Create testing 20% split of Food101\n",
        "test_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n",
        "                                                split_size=0.2)\n",
        "\n",
        "len(train_data_food101_20_percent), len(test_data_food101_20_percent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQa-CJ8qWmsZ",
        "outputId": "54076bf2-297e-4759-b41d-a720b996bffe"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Splitting dataset of length 75750 into splits of size: 15150 (20%), 60600 (80%)\n",
            "[INFO] Splitting dataset of length 25250 into splits of size: 5050 (20%), 20200 (80%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15150, 5050)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.4 Turning our Food101 datasets into `DataLoader`s\n",
        "\n",
        "Now let's turn our Food101 20% dataset splits into `DataLoader`'s using `torch.utils.data.DataLoader()`.\n",
        "\n",
        "We'll set `shuffle=True` for the training data only and the batch size to `32` for both datasets.\n",
        "\n",
        "And we'll set `num_workers` to `4` if the CPU count is available or `2` if it's not (though the value of `num_workers` is very experimental and will depend on the hardware you're using, there's an [active discussion thread about this on the PyTorch forums](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813))."
      ],
      "metadata": {
        "id": "b6P5p3fUW3_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2 if os.cpu_count() <= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs\n",
        "\n",
        "# Create Food101 20 percent training DataLoader\n",
        "train_dataloader_food101_20_percent = torch.utils.data.DataLoader(train_data_food101_20_percent,\n",
        "                                                                  batch_size=BATCH_SIZE,\n",
        "                                                                  shuffle=True,\n",
        "                                                                  num_workers=NUM_WORKERS)\n",
        "# Create Food101 20 percent testing DataLoader\n",
        "test_dataloader_food101_20_percent = torch.utils.data.DataLoader(test_data_food101_20_percent,\n",
        "                                                                 batch_size=BATCH_SIZE,\n",
        "                                                                 shuffle=False,\n",
        "                                                                 num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "wwRmYsumW380"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5 Training FoodVision Big model\n",
        "\n",
        "FoodVision Big model and `DataLoader`s ready!\n",
        "\n",
        "Time for training.\n",
        "\n",
        "We'll create an optimizer using `torch.optim.Adam()` and a learning rate of `1e-3`.\n",
        "\n",
        "And because we've got so many classes, we'll also setup a loss function using `torch.nn.CrossEntropyLoss()` with `label_smoothing=0.1`, inline with [`torchvision`'s state-of-the-art training recipe](https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#label-smoothing).\n",
        "\n",
        "What's [**label smoothing**](https://paperswithcode.com/method/label-smoothing)?\n",
        "\n",
        "Label smoothing is a regularization technique (regularization is another word to describe the process of [preventing overfitting](https://www.learnpytorch.io/04_pytorch_custom_datasets/#81-how-to-deal-with-overfitting)) that reduces the value a model gives to anyone label and spreads it across the other labels.\n",
        "\n",
        "In essence, rather than a model getting *too confident* on a single label, label smoothing gives a non-zero value to other labels to help aid in generalization.\n",
        "\n",
        "For example, if a model *without* label smoothing had the following outputs for 5 classes:\n",
        "\n",
        "```\n",
        "[0, 0, 0.99, 0.01, 0]\n",
        "```\n",
        "\n",
        "A model *with* label smoothing may have the following outputs:\n",
        "\n",
        "```\n",
        "[0.01, 0.01, 0.96, 0.01, 0.01]\n",
        "```\n",
        "\n",
        "The model is still confident on its prediction of class 3 but giving small values to the other labels forces the model to at least consider other options.\n",
        "\n",
        "Finally, to keep things quick, we'll train our model for five epochs using the `engine.train()` function we created in [05. PyTorch Going Modular section 4](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them) with the goal of beating the original Food101 paper's result of 56.4% accuracy on the test set.\n",
        "\n",
        "Let's train our biggest model yet!\n",
        "\n",
        "> **Note:** Running the cell below will take ~15-20 minutes to run on Google Colab. This is because it's training the biggest model with the largest amount of data we've used so far (15,150 training images, 5050 testing images). And it's a reason we decided to split 20% of the full Food101 dataset off before (so training didn't take over an hour)."
      ],
      "metadata": {
        "id": "DtmggXSjW352"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),\n",
        "                             lr=1e-3)\n",
        "\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # throw in a little label smoothing because so many classes\n",
        "\n",
        "# Want to beat original Food101 paper with 20% of data, need 56.4%+ acc on test dataset\n",
        "set_seeds()\n",
        "effnetb2_food101_results = engine.train(model=effnetb2_food101,\n",
        "                                        train_dataloader=train_dataloader_food101_20_percent,\n",
        "                                        test_dataloader=test_dataloader_food101_20_percent,\n",
        "                                        optimizer=optimizer,\n",
        "                                        loss_fn=loss_fn,\n",
        "                                        epochs=5,\n",
        "                                        device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "8e6c47aff53c4456beaddcc7addd427c",
            "621538333cfe490caf7e6dffee4fcf9f",
            "88b770ea96f34faf9bbdd4f37411721b",
            "8cbd4d4d6bae479caaa32e714d550da8",
            "577c098e7e70491fb207e23c7fb80b9e",
            "e3376436004847e6ac2729f4e26302c8",
            "9ef149a05b0847ac8e4ab91089d8b18c",
            "a6c4b5d04e3146e69aa5c56de17ee21a",
            "8b6e26ee8dcf477db376da659f0edecc",
            "f7ff13f640534c0cbfce1409f3170ebb",
            "224e5e85e8944a80af38520252ed4110"
          ]
        },
        "id": "ojnzdYZaW33F",
        "outputId": "720a28d6-219e-4df8-9ee6-fd168951b319"
      },
      "execution_count": 80,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e6c47aff53c4456beaddcc7addd427c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 3.6422 | train_acc: 0.2795 | test_loss: 2.7824 | test_acc: 0.4903\n",
            "Epoch: 2 | train_loss: 2.8626 | train_acc: 0.4410 | test_loss: 2.4729 | test_acc: 0.5334\n",
            "Epoch: 3 | train_loss: 2.6559 | train_acc: 0.4866 | test_loss: 2.3633 | test_acc: 0.5556\n",
            "Epoch: 4 | train_loss: 2.5455 | train_acc: 0.5124 | test_loss: 2.3022 | test_acc: 0.5751\n",
            "Epoch: 5 | train_loss: 2.4963 | train_acc: 0.5235 | test_loss: 2.2799 | test_acc: 0.5810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like we beat the original Food101 paper's results of 56.4% accuracy with only 20% of the training data (though we only evaluated on 20% of the testing data too, to fully replicate the results, we could evaluate on 100% of the testing data).\n",
        "\n",
        "That's the power of transfer learning!"
      ],
      "metadata": {
        "id": "0R5dqZ2lbssK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "# Check out the loss curves for FoodVision Big\n",
        "plot_loss_curves(effnetb2_food101_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "wnO1KQSoXL18",
        "outputId": "d665d2fd-2de9-48a9-dde0-5e5df624bd1a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like our regularization techniques (data augmentation and label smoothing) helped prevent our model from overfitting (the training loss is still higher than the test loss) this indicates our model has a bit more capacity to learn and could improve with further training."
      ],
      "metadata": {
        "id": "RI52aA5Qbxyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.7 Saving and loading FoodVision Big\n",
        "\n",
        "Now we've trained our biggest model yet, let's save it so we can load it back in later."
      ],
      "metadata": {
        "id": "JlDDU8PkbxwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "# Create a model path\n",
        "effnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"\n",
        "\n",
        "# Save FoodVision Big model\n",
        "utils.save_model(model=effnetb2_food101,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=effnetb2_food101_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EIvdgwJbxtu",
        "outputId": "ed28315e-f3f2-44e9-9274-301e5e8d6989"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to: models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model saved!\n",
        "\n",
        "Before we move on, let's make sure we can load it back in.\n",
        "\n",
        "We'll do so by creating a model instance first with `create_effnetb2_model(num_classes=101)` (101 classes for all Food101 classes).\n",
        "\n",
        "And then loading the saved `state_dict()` with [`torch.nn.Module.load_state_dict()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict) and [`torch.load()`](https://pytorch.org/docs/stable/generated/torch.load.html)."
      ],
      "metadata": {
        "id": "gRsY4AjqbxrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Food101 compatible EffNetB2 instance\n",
        "loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n",
        "\n",
        "# Load the saved model's state_dict()\n",
        "loaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZJ7a6GTbxpO",
        "outputId": "af6b09dc-9481-40a2-ae2e-e4a68f83cf2b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.8 Checking FoodVision Big model size\n",
        "\n",
        "Our FoodVision Big model is capable of classifying 101 classes versus FoodVision Mini's 3 classes, a 33.6x increase!\n",
        "\n",
        "How does this affect the model size?\n",
        "\n",
        "Let's find out."
      ],
      "metadata": {
        "id": "5uvRgqLvbxmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqrWFKRPbxj1",
        "outputId": "d5f9b391-af94-486e-cb76-00fc4f0acbf2"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained EffNetB2 feature extractor Food101 model size: 30 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmm, it looks like the model size stayed largely the same (30 MB for FoodVision Big and 29 MB for FoodVision Mini) despite the large increase in the number of classes.\n",
        "\n",
        "This is because all the extra parameters for FoodVision Big are *only* in the last layer (the classifier head).\n",
        "\n",
        "All of the base layers are the same between FoodVision Big and FoodVision Mini.\n",
        "\n",
        "Going back up and comparing the model summaries will give more details.\n",
        "\n",
        "| **Model** | **Output shape (num classes)** | **Trainable parameters** | **Total parameters** | **Model size (MB)** |\n",
        "| ----- | ----- | ----- | ----- | ----- |\n",
        "| FoodVision Mini (EffNetB2 feature extractor) | 3 | 4,227 | 7,705,221 |  29 |\n",
        "| FoodVision Big (EffNetB2 feature extractor) | 101 | 142,309 | 7,843,303 | 30 |\n"
      ],
      "metadata": {
        "id": "-o2L4df8bxhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Turning our FoodVision Big model into a deployable app\n",
        "\n",
        "We've got a trained and saved EffNetB2 model on 20% of the Food101 dataset.\n",
        "\n",
        "And instead of letting our model live in a folder all its life, let's deploy it!\n",
        "\n",
        "We'll deploy our FoodVision Big model in the same way we deployed our FoodVision Mini model, as a Gradio demo on Hugging Face Spaces.\n",
        "\n",
        "To begin, let's create a `demos/foodvision_big/` directory to store our FoodVision Big demo files as well as a `demos/foodvision_big/examples` directory to hold an example image to test the demo with.\n",
        "\n",
        "When we're finished we'll have the following file structure:\n",
        "\n",
        "```\n",
        "demos/\n",
        "  foodvision_big/\n",
        "    09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n",
        "    app.py\n",
        "    class_names.txt\n",
        "    examples/\n",
        "      example_1.jpg\n",
        "    model.py\n",
        "    requirements.txt\n",
        "```\n",
        "\n",
        "Where:\n",
        "* `09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth` is our trained PyTorch model file.\n",
        "* `app.py` contains our FoodVision Big Gradio app.\n",
        "* `class_names.txt` contains all of the class names for FoodVision Big.\n",
        "* `examples/` contains example images to use with our Gradio app.\n",
        "* `model.py` contains the model definition as well as any transforms associated with the model.\n",
        "* `requirements.txt` contains the dependencies to run our app such as `torch`, `torchvision` and `gradio`."
      ],
      "metadata": {
        "id": "NJ7areB3cDl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# create foodvision big demo path\n",
        "foodvision_big_demo_path = Path(\"demos/foodvision_big/\")\n",
        "\n",
        "# make foodvision vig demo directory\n",
        "foodvision_big_demo_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#make foodvision big demo examples directory\n",
        "(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "cA_m089_cDjI"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.1 Downloading an example image and moving it to the `examples` directory\n",
        "\n",
        "For our example image, we're going to use the faithful [`https://github.com/angkonn/PyTorch-for-Deep-Learning-Machine-Learning/blob/main/images/pizza_custom.jpeg]\n",
        "\n",
        "So let's download it from the course GitHub via the `!wget` command and then we can move it to `demos/foodvision_big/examples` with the `!mv` command (short for \"move\").\n",
        "\n",
        "While we're here we'll move our trained Food101 EffNetB2 model from `models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth` to `demos/foodvision_big` as well."
      ],
      "metadata": {
        "id": "XdOMeiAxcDgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and move an example image\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\n",
        "!mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpg\n",
        "\n",
        "# Move trained model to FoodVision Big demo folder (will error if model is already moved)\n",
        "!mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc8anfILcDc-",
        "outputId": "cbbe33d5-2446-4cc2-99fa-86fbabb26726"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-21 08:29:11--  https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2874848 (2.7M) [image/jpeg]\n",
            "Saving to: ‘04-pizza-dad.jpeg’\n",
            "\n",
            "\r04-pizza-dad.jpeg     0%[                    ]       0  --.-KB/s               \r04-pizza-dad.jpeg   100%[===================>]   2.74M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-21 08:29:11 (51.8 MB/s) - ‘04-pizza-dad.jpeg’ saved [2874848/2874848]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2 Saving Food101 class names to file (`class_names.txt`)\n",
        "\n",
        "Because there are so many classes in the Food101 dataset, instead of storing them as a list in our `app.py` file, let's save them to a `.txt` file and read them in when necessary instead.\n",
        "\n",
        "We'll just remind ourselves what they look like first by checking out `food101_class_names`."
      ],
      "metadata": {
        "id": "T2E1goWHd4zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the first 10 Food101 class names\n",
        "food101_class_names[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNsT0-TYd6gM",
        "outputId": "de5bf405-15c6-4e8c-8ac7-681ac2707df5"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apple_pie',\n",
              " 'baby_back_ribs',\n",
              " 'baklava',\n",
              " 'beef_carpaccio',\n",
              " 'beef_tartare',\n",
              " 'beet_salad',\n",
              " 'beignets',\n",
              " 'bibimbap',\n",
              " 'bread_pudding',\n",
              " 'breakfast_burrito']"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create path to Food101 class names\n",
        "foodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n",
        "\n",
        "# Write Food101 class names list to file\n",
        "with open(foodvision_big_class_names_path, \"w\") as f:\n",
        "    print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")\n",
        "    f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBE0Sn5vd--j",
        "outputId": "7a0dee9c-76ea-4234-e55b-a6458adf5855"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving Food101 class names to demos/foodvision_big/class_names.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open Food101 class names file and read each line into a list\n",
        "with open(foodvision_big_class_names_path, \"r\") as f:\n",
        "    food101_class_names_loaded = [food.strip() for food in  f.readlines()]\n",
        "\n",
        "# View the first 5 class names loaded back in\n",
        "food101_class_names_loaded[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcnZ7nJEeDfU",
        "outputId": "fd891023-515d-4980-95d4-220d0376312a"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.3 Turning our FoodVision Big model into a Python script (`model.py`)\n",
        "\n",
        "Just like the FoodVision Mini demo, let's create a script that's capable of instantiating an EffNetB2 feature extractor model along with its necessary transforms."
      ],
      "metadata": {
        "id": "EymHaM6BeHGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=3,\n",
        "                          seed:int=42):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of classes in the classifier head.\n",
        "            Defaults to 3.\n",
        "        seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): EffNetB2 feature extractor model.\n",
        "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "    \"\"\"\n",
        "    # Create EffNetB2 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8J8B7g8eIoc",
        "outputId": "73ba3bb5-4afc-4439-f09e-58d40f5d2214"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_big/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4 Turning our FoodVision Big Gradio app into a Python script (`app.py`)\n",
        "\n",
        "We've got a FoodVision Big `model.py` script, now let's create a FoodVision Big `app.py` script.\n",
        "\n",
        "This will again mostly be the same as the FoodVision Mini `app.py` script except we'll change:\n",
        "\n",
        "1. **Imports and class names setup** - The `class_names` variable will be a list for all of the Food101 classes rather than pizza, steak, sushi. We can access these via `demos/foodvision_big/class_names.txt`.\n",
        "2. **Model and transforms preparation** - The `model` will have `num_classes=101` rather than `num_classes=3`. We'll also be sure to load the weights from `\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"` (our FoodVision Big model path).\n",
        "3. **Predict function** - This will stay the same as FoodVision Mini's `app.py`.\n",
        "4. **Gradio app** - The Gradio interface will have different `title`, `description` and `article` parameters to reflect the details of FoodVision Big.\n",
        "\n",
        "We'll also make sure to save it to `demos/foodvision_big/app.py` using the `%%writefile` magic command."
      ],
      "metadata": {
        "id": "3ZEWjzlHeO4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/app.py\n",
        "### 1. Imports and class names setup ###\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Setup class names\n",
        "with open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt\n",
        "    class_names = [food_name.strip() for food_name in  f.readlines()]\n",
        "\n",
        "### 2. Model and transforms preparation ###\n",
        "\n",
        "# Create model\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
        "    num_classes=101, # could also use len(class_names)\n",
        ")\n",
        "\n",
        "# Load saved weights\n",
        "effnetb2.load_state_dict(\n",
        "    torch.load(\n",
        "        f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",\n",
        "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
        "    )\n",
        ")\n",
        "\n",
        "### 3. Predict function ###\n",
        "\n",
        "# Create predict function\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "    \"\"\"\n",
        "    # Start the timer\n",
        "    start_time = timer()\n",
        "\n",
        "    # Transform the target image and add a batch dimension\n",
        "    img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "    # Put model into evaluation mode and turn on inference mode\n",
        "    effnetb2.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
        "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
        "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "    # Calculate the prediction time\n",
        "    pred_time = round(timer() - start_time, 5)\n",
        "\n",
        "    # Return the prediction dictionary and prediction time\n",
        "    return pred_labels_and_probs, pred_time\n",
        "\n",
        "### 4. Gradio app ###\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"FoodVision Big 🍔👁\"\n",
        "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food into [101 different classes](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\"\n",
        "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
        "\n",
        "# Create examples list from \"examples/\" directory\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=[\n",
        "        gr.Label(num_top_classes=5, label=\"Predictions\"),\n",
        "        gr.Number(label=\"Prediction time (s)\"),\n",
        "    ],\n",
        "    examples=example_list,\n",
        "    title=title,\n",
        "    description=description,\n",
        "    article=article,\n",
        ")\n",
        "\n",
        "# Launch the app!\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "036UqA8keRRl",
        "outputId": "708f43cd-6c6c-47ab-fbda-3c10d044c1e2"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_big/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.5 Creating a requirements file for FoodVision Big (`requirements.txt`)\n",
        "\n",
        "Now all we need is a `requirements.txt` file to tell our Hugging Face Space what dependencies our FoodVision Big app requires."
      ],
      "metadata": {
        "id": "-ykkzaYleT-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/requirements.txt\n",
        "torch\n",
        "torchvision\n",
        "httpx==0.24.1\n",
        "httpcore==0.17.0\n",
        "gradio\n",
        "huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk1Olwo5eZ-m",
        "outputId": "e99f8063-a1a2-4eb0-a350-4e08f94bb264"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_big/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.6 Downloading our FoodVision Big app files\n",
        "\n",
        "We've got all the files we need to deploy our FoodVision Big app on Hugging Face, let's now zip them together and download them.\n",
        "\n",
        "We'll use the same process we used for the FoodVision Mini app above in [section 9.1: *Downloading our Foodvision Mini app files*](https://www.learnpytorch.io/09_pytorch_model_deployment/#91-downloading-our-foodvision-mini-app-files)."
      ],
      "metadata": {
        "id": "c-5V1abfef9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip foodvision_big folder but exclude certain files\n",
        "!cd demos/foodvision_big && zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
        "\n",
        "# Download the zipped FoodVision Big app (if running in Google Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(\"demos/foodvision_big.zip\")\n",
        "except:\n",
        "    print(\"Not running in Google Colab, can't use google.colab.files.download()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "ryujHpwSehfH",
        "outputId": "2bbe3933-e687-4acf-df0d-bb2918b5919d"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: 09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth (deflated 8%)\n",
            "  adding: app.py (deflated 54%)\n",
            "  adding: class_names.txt (deflated 48%)\n",
            "  adding: examples/ (stored 0%)\n",
            "  adding: examples/04-pizza-dad.jpg (deflated 0%)\n",
            "  adding: model.py (deflated 56%)\n",
            "  adding: requirements.txt (deflated 8%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_166d5fb5-22d7-4b7f-826d-b2538e64302b\", \"foodvision_big.zip\", 32189566)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "87a7876cfe054d01aa15f44dc4b93ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86a2cc37693c4ba8a273968fc6ae5050",
              "IPY_MODEL_bc3a18d069214cbe89cbe4b153e4f193",
              "IPY_MODEL_5e53ffc3c15d43e885721a3f34d41d9f"
            ],
            "layout": "IPY_MODEL_91a17accaf7549f6aee235674e28d9ac"
          }
        },
        "86a2cc37693c4ba8a273968fc6ae5050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67a2f11c7d09408dac0d18c3cc37b064",
            "placeholder": "​",
            "style": "IPY_MODEL_0fe2f9569f8141adbf55e635e0df9dca",
            "value": "100%"
          }
        },
        "bc3a18d069214cbe89cbe4b153e4f193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66ff84bc57f148c9bdf37d38cc3d4c8d",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bfbcccc7c5f44417906eefcc629a6f7a",
            "value": 10
          }
        },
        "5e53ffc3c15d43e885721a3f34d41d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2fb5c154b4e44f282ab431dbac2c663",
            "placeholder": "​",
            "style": "IPY_MODEL_3552726fe5b540acad5daa1faa5a1612",
            "value": " 10/10 [01:02&lt;00:00,  5.48s/it]"
          }
        },
        "91a17accaf7549f6aee235674e28d9ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67a2f11c7d09408dac0d18c3cc37b064": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fe2f9569f8141adbf55e635e0df9dca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66ff84bc57f148c9bdf37d38cc3d4c8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfbcccc7c5f44417906eefcc629a6f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2fb5c154b4e44f282ab431dbac2c663": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3552726fe5b540acad5daa1faa5a1612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7db65f779c14d8c80dd38dea3215c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c40b25147dd42219d09257a1631ce2c",
              "IPY_MODEL_e9165874ee7a451ca285afe2a72641f7",
              "IPY_MODEL_658fac3786d24ff89f47fef49baa2f71"
            ],
            "layout": "IPY_MODEL_3ab4f077c5c4466e864d4edb04a4145c"
          }
        },
        "5c40b25147dd42219d09257a1631ce2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdcffa90dac24f358d2a31076fa055ed",
            "placeholder": "​",
            "style": "IPY_MODEL_81bb1c23dafa46d8ba936575925993e8",
            "value": "100%"
          }
        },
        "e9165874ee7a451ca285afe2a72641f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41998295434947b680d14db02b982371",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b9435b544324446b478d57e2c016b1c",
            "value": 10
          }
        },
        "658fac3786d24ff89f47fef49baa2f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_523357fcff054e06b028bfc329ac6b22",
            "placeholder": "​",
            "style": "IPY_MODEL_205e36d46196400ba4fe8da2c8a9653e",
            "value": " 10/10 [01:25&lt;00:00,  8.80s/it]"
          }
        },
        "3ab4f077c5c4466e864d4edb04a4145c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdcffa90dac24f358d2a31076fa055ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81bb1c23dafa46d8ba936575925993e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41998295434947b680d14db02b982371": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b9435b544324446b478d57e2c016b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "523357fcff054e06b028bfc329ac6b22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "205e36d46196400ba4fe8da2c8a9653e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e6c47aff53c4456beaddcc7addd427c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_621538333cfe490caf7e6dffee4fcf9f",
              "IPY_MODEL_88b770ea96f34faf9bbdd4f37411721b",
              "IPY_MODEL_8cbd4d4d6bae479caaa32e714d550da8"
            ],
            "layout": "IPY_MODEL_577c098e7e70491fb207e23c7fb80b9e"
          }
        },
        "621538333cfe490caf7e6dffee4fcf9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3376436004847e6ac2729f4e26302c8",
            "placeholder": "​",
            "style": "IPY_MODEL_9ef149a05b0847ac8e4ab91089d8b18c",
            "value": "100%"
          }
        },
        "88b770ea96f34faf9bbdd4f37411721b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6c4b5d04e3146e69aa5c56de17ee21a",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b6e26ee8dcf477db376da659f0edecc",
            "value": 5
          }
        },
        "8cbd4d4d6bae479caaa32e714d550da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7ff13f640534c0cbfce1409f3170ebb",
            "placeholder": "​",
            "style": "IPY_MODEL_224e5e85e8944a80af38520252ed4110",
            "value": " 5/5 [16:14&lt;00:00, 192.82s/it]"
          }
        },
        "577c098e7e70491fb207e23c7fb80b9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3376436004847e6ac2729f4e26302c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ef149a05b0847ac8e4ab91089d8b18c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6c4b5d04e3146e69aa5c56de17ee21a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b6e26ee8dcf477db376da659f0edecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7ff13f640534c0cbfce1409f3170ebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "224e5e85e8944a80af38520252ed4110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}