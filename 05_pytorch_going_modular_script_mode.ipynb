{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOtemTdOJk4sI2quxZNvUuS","collapsed_sections":[],"include_colab_link":true,"name":"05_pytorch_going_modular_script_mode.ipynb","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"1e79615e722f456189e07874d13564d2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3402f79d3df643efbed25c72aac94642":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4bc100911d7d4303b6ab7efb59d3420c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d780e75e5d24241b1f6c0203a8bd73e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9b7fbae53e84883ac281343bb6a9da9","placeholder":"​","style":"IPY_MODEL_4bc100911d7d4303b6ab7efb59d3420c","value":" 5/5 [00:11&lt;00:00,  2.23s/it]"}},"83cf71b79abd4fc5ae9daaac8f586841":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84317b054c244ee4b1bed7b2f86660b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0a72aed7e2743f8a95c6af3b6e4a457","IPY_MODEL_b517f855551f4c53969677161952a1ac","IPY_MODEL_4d780e75e5d24241b1f6c0203a8bd73e"],"layout":"IPY_MODEL_a0caf8f4760c4935b5b9be86566ee8e0"}},"a0a72aed7e2743f8a95c6af3b6e4a457":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83cf71b79abd4fc5ae9daaac8f586841","placeholder":"​","style":"IPY_MODEL_3402f79d3df643efbed25c72aac94642","value":"100%"}},"a0caf8f4760c4935b5b9be86566ee8e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b517f855551f4c53969677161952a1ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e79615e722f456189e07874d13564d2","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bac27d87b4d54b6cbaf85dd7b7ecdc64","value":5}},"bac27d87b4d54b6cbaf85dd7b7ecdc64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9b7fbae53e84883ac281343bb6a9da9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/going_modular/05_pytorch_going_modular_script_mode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"colab_type":"text","id":"view-in-github"}},{"cell_type":"markdown","source":"# 05. Going Modular: Part 2 (script mode)\n\nThis notebook is part 2/2 of section [05. Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n\nFor reference, the two parts are: \n1. [**05. Going Modular: Part 1 (cell mode)**](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/05_pytorch_going_modular_cell_mode.ipynb) - this notebook is run as a traditional Jupyter Notebook/Google Colab notebook and is a condensed version of [notebook 04](https://www.learnpytorch.io/04_pytorch_custom_datasets/).\n2. [**05. Going Modular: Part 2 (script mode)**](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/05_pytorch_going_modular_script_mode.ipynb) - this notebook is the same as number 1 but with added functionality to turn each of the major sections into Python scripts, such as, `data_setup.py` and `train.py`. \n\nWhy two parts?\n\nBecause sometimes the best way to learn something is to see how it *differs* from something else.\n\nIf you run each notebook side-by-side you'll see how they differ and that's where the key learnings are.","metadata":{"id":"1M5tsObEa17-","tags":[]}},{"cell_type":"markdown","source":"## What is script mode?\n\n**Script mode** uses [Jupyter Notebook cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html) (special commands) to turn specific cells into Python scripts.\n\nFor example if you run the following code in a cell, you'll create a Python file called `hello_world.py`:\n\n```\n%%writefile hello_world.py\nprint(\"hello world, machine learning is fun!\")\n```\n\nYou could then run this Python file on the command line with:\n\n```\npython hello_world.py\n\n>>> hello world, machine learning is fun!\n```\n\nThe main cell magic we're interested in using is `%%writefile`.\n\nPutting `%%writefile filename` at the top of a cell in Jupyter or Google Colab will write the contents of that cell to a specified `filename`.\n\n> **Question:** Do I have to create Python files like this? Can't I just start directly with a Python file and skip using a Google Colab notebook?\n>\n> **Answer:** Yes. This is only *one* way of creating Python scripts. If you know the kind of script you'd like to write, you could start writing it straight away. But since using Jupyter/Google Colab notebooks is a popular way of starting off data science and machine learning projects, knowing about the `%%writefile` magic command is a handy tip. ","metadata":{"id":"1M5tsObEa17-","tags":[]}},{"cell_type":"markdown","source":"## What has script mode got to do with PyTorch?\n\nIf you've written some useful code in a Jupyter Notebook or Google Colab notebook, chances are you'll want to use that code again.\n\nAnd turning your useful cells into Python scripts (`.py` files) means you can use specific pieces of your code in other projects.\n\nThis practice is not PyTorch specific.\n\nBut it's how you'll see many different online PyTorch repositories structured.","metadata":{"id":"1M5tsObEa17-","tags":[]}},{"cell_type":"markdown","source":"### PyTorch in the wild\n\nFor example, if you find a PyTorch project on GitHub, it may be structured in the following way:\n\n```\npytorch_project/\n├── pytorch_project/\n│   ├── data_setup.py\n│   ├── engine.py\n│   ├── model.py\n│   ├── train.py\n│   └── utils.py\n├── models/\n│   ├── model_1.pth\n│   └── model_2.pth\n└── data/\n    ├── data_folder_1/\n    └── data_folder_2/\n```\n\nHere, the top level directory is called `pytorch_project` but you could call it whatever you want.\n\nInside there's another directory called `pytorch_project` which contains several `.py` files, the purposes of these may be:\n* `data_setup.py` - a file to prepare data (and download data if needed).\n* `engine.py` - a file containing various training functions.\n* `model_builder.py` or `model.py` - a file to create a PyTorch model.\n* `train.py` - a file to leverage all other files and train a target PyTorch model.\n* `utils.py` - a file dedicated to helpful utility functions.\n\nAnd the `models` and `data` directories could hold PyTorch models and data files respectively (though due to the size of models and data files, it's unlikely you'll find the *full* versions of these on GitHub, these directories are present above mainly for demonstration purposes).\n\n> **Note:** There are many different ways to structure a Python project and subsequently a PyTorch project. This isn't a guide on *how* to structure your projects, only an example of how you *might* come across PyTorch projects in the wild. For more on structuring Python projects, see Real Python's [*Python Application Layouts: A Reference*](https://realpython.com/python-application-layouts/) guide.","metadata":{"id":"1M5tsObEa17-","tags":[]}},{"cell_type":"markdown","source":"## What's the difference between this notebook (Part 2) and the cell mode notebook (Part 1)?\n\nThis notebook, 05 Going Modular: Part 2 (script mode), creates Python scripts out of the cells created in part 1.\n\nRunning this notebook end-to-end will result in having a directory structure very similar to the `pytorch_project` structure above.\n\nYou'll notice each section in Part 2 (script mode) has an extra subsection (e.g. 2.1, 3.1, 4.1) for turning cell code into script code.","metadata":{"id":"1M5tsObEa17-","tags":[]}},{"cell_type":"markdown","source":"## What we're going to cover\n\nBy the end of this notebook you should finish with a directory structure of: \n\n```\ngoing_modular/\n├── going_modular/\n│   ├── data_setup.py\n│   ├── engine.py\n│   ├── model_builder.py\n│   ├── train.py\n│   └── utils.py\n├── models/\n│   ├── 05_going_modular_cell_mode_tinyvgg_model.pth\n│   └── 05_going_modular_script_mode_tinyvgg_model.pth\n└── data/\n    └── pizza_steak_sushi/\n        ├── train/\n        │   ├── pizza/\n        │   │   ├── image01.jpeg\n        │   │   └── ...\n        │   ├── steak/\n        │   └── sushi/\n        └── test/\n            ├── pizza/\n            ├── steak/\n            └── sushi/\n```\n\nUsing this directory structure, you should be able to train a model from within a notebook with the command:\n\n```\n!python going_modular/train.py\n```\n\nOr from the command line with:\n\n```\npython going_modular/train.py\n```\n\nIn essence, we will have turned our helpful notebook code into **reusable modular code**.","metadata":{"id":"1M5tsObEa17-","tags":[]}},{"cell_type":"markdown","source":"## Where can you get help?\n\nYou can find the book version of this section [05. PyTorch Going Modular on learnpytorch.io](https://www.learnpytorch.io/05_pytorch_going_modular/).\n\nThe rest of the materials for this course [are available on GitHub](https://github.com/mrdbourke/pytorch-deep-learning).\n\nIf you run into trouble, you can ask a question on the course [GitHub Discussions page](https://github.com/mrdbourke/pytorch-deep-learning/discussions).\n\nAnd of course, there's the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) and [PyTorch developer forums](https://discuss.pytorch.org/), a very helpful place for all things PyTorch. ","metadata":{"id":"1M5tsObEa17-","tags":[]}},{"cell_type":"markdown","source":"## 0. Creating a folder for storing Python scripts\n\nSince we're going to be creating Python scripts out of our most useful code cells, let's create a folder for storing those scripts.\n\nWe'll call the folder `going_modular` and create it using Python's [`os.makedirs()`](https://docs.python.org/3/library/os.html) method.","metadata":{"id":"OW_3D_fOap2I"}},{"cell_type":"code","source":"import os\n\nos.makedirs(\"going_modular\", exist_ok=True)","metadata":{"id":"K29qwItveNm9","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:35.003728Z","iopub.execute_input":"2025-04-19T06:18:35.004000Z","iopub.status.idle":"2025-04-19T06:18:35.008119Z","shell.execute_reply.started":"2025-04-19T06:18:35.003981Z","shell.execute_reply":"2025-04-19T06:18:35.007303Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## 1. Get data\n\nWe're going to start by downloading the same data we used in [notebook 04](https://www.learnpytorch.io/04_pytorch_custom_datasets/#1-get-data), the `pizza_steak_sushi` dataset with images of pizza, steak and sushi.","metadata":{"id":"1XgXFr97bCsI"}},{"cell_type":"code","source":"import os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n# Download pizza, steak, sushi data\nwith open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n    print(\"Downloading pizza, steak, sushi data...\")\n    f.write(request.content)\n\n# Unzip pizza, steak, sushi data\nwith zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n    print(\"Unzipping pizza, steak, sushi data...\") \n    zip_ref.extractall(image_path)\n\n# Remove zip file\nos.remove(data_path / \"pizza_steak_sushi.zip\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wl_jfPzVbGDS","outputId":"51a3e5e8-991c-4309-b1ba-b24d32bbdb61","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:35.009579Z","iopub.execute_input":"2025-04-19T06:18:35.009942Z","iopub.status.idle":"2025-04-19T06:18:35.678373Z","shell.execute_reply.started":"2025-04-19T06:18:35.009919Z","shell.execute_reply":"2025-04-19T06:18:35.677460Z"}},"outputs":[{"name":"stdout","text":"Did not find data/pizza_steak_sushi directory, creating one...\nDownloading pizza, steak, sushi data...\nUnzipping pizza, steak, sushi data...\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Setup train and testing paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\ntrain_dir, test_dir","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8qYMdiBMbde0","outputId":"a5093c72-455f-471c-9e95-5b391eea9ebf","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:35.679166Z","iopub.execute_input":"2025-04-19T06:18:35.679433Z","iopub.status.idle":"2025-04-19T06:18:35.685808Z","shell.execute_reply.started":"2025-04-19T06:18:35.679412Z","shell.execute_reply":"2025-04-19T06:18:35.685093Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(PosixPath('data/pizza_steak_sushi/train'),\n PosixPath('data/pizza_steak_sushi/test'))"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## 2. Create Datasets and DataLoaders\n\nLet's turn our data into PyTorch `Dataset`'s and `DataLoader`'s and find out a few useful attributes from them such as `classes` and their lengths. ","metadata":{"id":"eYL6NynybU9Z"}},{"cell_type":"code","source":"from torchvision import datasets, transforms\n\n# Create simple transform\ndata_transform = transforms.Compose([ \n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n])\n\n# Use ImageFolder to create dataset(s)\ntrain_data = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\ntest_data = datasets.ImageFolder(root=test_dir, \n                                 transform=data_transform)\n\nprint(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DaOJOd-QbOQk","outputId":"dc7168b3-8bee-40c4-867e-58aa058175b5","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:35.686464Z","iopub.execute_input":"2025-04-19T06:18:35.686687Z","iopub.status.idle":"2025-04-19T06:18:43.666043Z","shell.execute_reply.started":"2025-04-19T06:18:35.686669Z","shell.execute_reply":"2025-04-19T06:18:43.665152Z"}},"outputs":[{"name":"stdout","text":"Train data:\nDataset ImageFolder\n    Number of datapoints: 225\n    Root location: data/pizza_steak_sushi/train\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n               ToTensor()\n           )\nTest data:\nDataset ImageFolder\n    Number of datapoints: 75\n    Root location: data/pizza_steak_sushi/test\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n               ToTensor()\n           )\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Get class names as a list\nclass_names = train_data.classes\nclass_names","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eEln0VFmbfyY","outputId":"c514574e-e3ab-4689-a5ee-6a2aef6a11d2","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:43.668219Z","iopub.execute_input":"2025-04-19T06:18:43.668632Z","iopub.status.idle":"2025-04-19T06:18:43.675192Z","shell.execute_reply.started":"2025-04-19T06:18:43.668608Z","shell.execute_reply":"2025-04-19T06:18:43.674228Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['pizza', 'steak', 'sushi']"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Can also get class names as a dict\nclass_dict = train_data.class_to_idx\nclass_dict","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMGTFVr8bgwE","outputId":"4268d11f-8899-447a-baba-7e91ade01f13","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:43.676466Z","iopub.execute_input":"2025-04-19T06:18:43.676786Z","iopub.status.idle":"2025-04-19T06:18:43.695312Z","shell.execute_reply.started":"2025-04-19T06:18:43.676759Z","shell.execute_reply":"2025-04-19T06:18:43.694459Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'pizza': 0, 'steak': 1, 'sushi': 2}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Check the lengths\nlen(train_data), len(test_data)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6a8IW-IYbhGX","outputId":"87bc60ac-3be3-499f-e0a8-aed5964a3075","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:43.696229Z","iopub.execute_input":"2025-04-19T06:18:43.696935Z","iopub.status.idle":"2025-04-19T06:18:43.718864Z","shell.execute_reply.started":"2025-04-19T06:18:43.696909Z","shell.execute_reply":"2025-04-19T06:18:43.718072Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(225, 75)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Turn train and test Datasets into DataLoaders\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(dataset=train_data, \n                              batch_size=1, # how many samples per batch?\n                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n                              shuffle=True) # shuffle the data?\n\ntest_dataloader = DataLoader(dataset=test_data, \n                             batch_size=1, \n                             num_workers=1, \n                             shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader, test_dataloader","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1UxnvvmblUj","outputId":"1dd0c011-912e-498b-ea34-ee4bf05c7b87","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:43.719777Z","iopub.execute_input":"2025-04-19T06:18:43.720348Z","iopub.status.idle":"2025-04-19T06:18:43.738583Z","shell.execute_reply.started":"2025-04-19T06:18:43.720320Z","shell.execute_reply":"2025-04-19T06:18:43.737769Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(<torch.utils.data.dataloader.DataLoader at 0x795214208f50>,\n <torch.utils.data.dataloader.DataLoader at 0x79521421c550>)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Check out single image size/shape\nimg, label = next(iter(train_dataloader))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJKRqXNGbnI5","outputId":"2d5320fc-ac72-4ef8-94f8-0ef283fc3a30","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:43.739513Z","iopub.execute_input":"2025-04-19T06:18:43.739817Z","iopub.status.idle":"2025-04-19T06:18:43.957200Z","shell.execute_reply.started":"2025-04-19T06:18:43.739798Z","shell.execute_reply":"2025-04-19T06:18:43.956338Z"}},"outputs":[{"name":"stdout","text":"Image shape: torch.Size([1, 3, 64, 64]) -> [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### 2.1 Create Datasets and DataLoaders (script mode)\n\nRather than rewriting all of the code above everytime we wanted to load data, we can turn it into a script called `data_setup.py`.\n\nLet's capture all of the above functionality into a function called `create_dataloaders()`.","metadata":{"id":"RjINjcyGdwL9"}},{"cell_type":"code","source":"%%writefile going_modular/data_setup.py\n\"\"\"\nContains functionality for creating PyTorch DataLoaders for \nimage classification data.\n\"\"\"\nimport os\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nNUM_WORKERS = os.cpu_count()\n\ndef create_dataloaders(\n    train_dir: str, \n    test_dir: str, \n    transform: transforms.Compose, \n    batch_size: int, \n    num_workers: int=NUM_WORKERS\n):\n  \"\"\"Creates training and testing DataLoaders.\n\n  Takes in a training directory and testing directory path and turns\n  them into PyTorch Datasets and then into PyTorch DataLoaders.\n\n  Args:\n    train_dir: Path to training directory.\n    test_dir: Path to testing directory.\n    transform: torchvision transforms to perform on training and testing data.\n    batch_size: Number of samples per batch in each of the DataLoaders.\n    num_workers: An integer for number of workers per DataLoader.\n\n  Returns:\n    A tuple of (train_dataloader, test_dataloader, class_names).\n    Where class_names is a list of the target classes.\n    Example usage:\n      train_dataloader, test_dataloader, class_names = \\\n        = create_dataloaders(train_dir=path/to/train_dir,\n                             test_dir=path/to/test_dir,\n                             transform=some_transform,\n                             batch_size=32,\n                             num_workers=4)\n  \"\"\"\n  # Use ImageFolder to create dataset(s)\n  train_data = datasets.ImageFolder(train_dir, transform=transform)\n  test_data = datasets.ImageFolder(test_dir, transform=transform)\n\n  # Get class names\n  class_names = train_data.classes\n\n  # Turn images into data loaders\n  train_dataloader = DataLoader(\n      train_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n  test_dataloader = DataLoader(\n      test_data,\n      batch_size=batch_size,\n      shuffle=False,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n\n  return train_dataloader, test_dataloader, class_names","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CmjK-b04du_-","outputId":"07d3450d-bc24-4aa4-a6d6-57d60bf11416","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:43.958246Z","iopub.execute_input":"2025-04-19T06:18:43.958513Z","iopub.status.idle":"2025-04-19T06:18:43.965815Z","shell.execute_reply.started":"2025-04-19T06:18:43.958488Z","shell.execute_reply":"2025-04-19T06:18:43.965028Z"}},"outputs":[{"name":"stdout","text":"Writing going_modular/data_setup.py\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"%%writefile going_modular/pred_and_store.py\nimport pathlib\nimport torch\nimport torchvision.transforms\n\nfrom PIL import Image\nfrom timeit import default_timer as timer \nfrom tqdm.auto import tqdm\nfrom typing import List, Dict\n\n# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\ndef pred_and_store(paths: List[pathlib.Path], \n                   model: torch.nn.Module,\n                   transform: torchvision.transforms, \n                   class_names: List[str], \n                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n    \n    # 2. Create an empty list to store prediction dictionaries\n    pred_list = []\n    \n    # 3. Loop through target paths\n    for path in tqdm(paths):\n        \n        # 4. Create empty dictionary to store prediction information for each sample\n        pred_dict = {}\n\n        # 5. Get the sample path and ground truth class name\n        pred_dict[\"image_path\"] = path\n        class_name = path.parent.stem\n        pred_dict[\"class_name\"] = class_name\n        \n        # 6. Start the prediction timer\n        start_time = timer()\n        \n        # 7. Open image path\n        img = Image.open(path)\n        \n        # 8. Transform the image, add batch dimension and put image on target device\n        transformed_image = transform(img).unsqueeze(0).to(device) \n        \n        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n        model.to(device)\n        model.eval()\n        \n        # 10. Get prediction probability, predicition label and prediction class\n        with torch.inference_mode():\n            pred_logit = model(transformed_image) # perform inference on target sample \n            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n\n            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on) \n            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n            pred_dict[\"pred_class\"] = pred_class\n            \n            # 12. End the timer and calculate time per pred\n            end_time = timer()\n            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n\n        # 13. Does the pred match the true label?\n        pred_dict[\"correct\"] = class_name == pred_class\n\n        # 14. Add the dictionary to the list of preds\n        pred_list.append(pred_dict)\n    \n    # 15. Return list of prediction dictionaries\n    return pred_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:43.966760Z","iopub.execute_input":"2025-04-19T06:18:43.967028Z","iopub.status.idle":"2025-04-19T06:18:43.988030Z","shell.execute_reply.started":"2025-04-19T06:18:43.967002Z","shell.execute_reply":"2025-04-19T06:18:43.987236Z"}},"outputs":[{"name":"stdout","text":"Writing going_modular/pred_and_store.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 3. Making a model (TinyVGG) \n\nWe're going to use the same model we used in notebook 04: TinyVGG from the CNN Explainer website.\n\nThe only change here from notebook 04 is that a docstring has been added using [Google's Style Guide for Python](https://google.github.io/styleguide/pyguide.html#384-classes). ","metadata":{"id":"gAQLp1dCbrY0"}},{"cell_type":"code","source":"import torch\n\nfrom torch import nn \n\nclass TinyVGG(nn.Module):\n    \"\"\"Creates the TinyVGG architecture.\n\n    Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n    See the original architecture here: https://poloclub.github.io/cnn-explainer/\n\n    Args:\n    input_shape: An integer indicating number of input channels.\n    hidden_units: An integer indicating number of hidden units between layers.\n    output_shape: An integer indicating number of output units.\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n          nn.Conv2d(in_channels=input_shape, \n                    out_channels=hidden_units, \n                    kernel_size=3, \n                    stride=1, \n                    padding=0),  \n          nn.ReLU(),\n          nn.Conv2d(in_channels=hidden_units, \n                    out_channels=hidden_units,\n                    kernel_size=3,\n                    stride=1,\n                    padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2,\n                        stride=2)\n        )\n        self.conv_block_2 = nn.Sequential(\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n          nn.Flatten(),\n          # Where did this in_features shape come from? \n          # It's because each layer of our network compresses and changes the shape of our inputs data.\n          nn.Linear(in_features=hidden_units*13*13,\n                    out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.conv_block_1(x)\n        x = self.conv_block_2(x)\n        x = self.classifier(x)\n        return x\n        # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion","metadata":{"id":"Z6i2xWcwbvom","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:43.988920Z","iopub.execute_input":"2025-04-19T06:18:43.989163Z","iopub.status.idle":"2025-04-19T06:18:44.007571Z","shell.execute_reply.started":"2025-04-19T06:18:43.989143Z","shell.execute_reply":"2025-04-19T06:18:44.006626Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Now let's create an instance of `TinyVGG` and put it on the target device.\n\n> **Note:** If you're using Google Colab, and you'd like to use a GPU (recommended), you can turn one on via going to Runtime -> Change runtime type -> Hardware accelerator -> GPU.","metadata":{"id":"ROZsju1d9Cbu"}},{"cell_type":"code","source":"import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Instantiate an instance of the model\ntorch.manual_seed(42)\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\nmodel_0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tXOOAEHHc6-L","outputId":"3207fda6-896b-4cb5-9f08-f1e46879bf75","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:44.010519Z","iopub.execute_input":"2025-04-19T06:18:44.010811Z","iopub.status.idle":"2025-04-19T06:18:44.040714Z","shell.execute_reply.started":"2025-04-19T06:18:44.010788Z","shell.execute_reply":"2025-04-19T06:18:44.039810Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=1690, out_features=3, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"Let's check out our model by doing a dummy forward pass.","metadata":{"id":"C1CwvPiMcEfZ"}},{"cell_type":"code","source":"# 1. Get a batch of images and labels from the DataLoader\nimg_batch, label_batch = next(iter(train_dataloader))\n\n# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\nimg_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\nprint(f\"Single image shape: {img_single.shape}\\n\")\n\n# 3. Perform a forward pass on a single image\nmodel_0.eval()\nwith torch.inference_mode():\n    pred = model_0(img_single.to(device))\n    \n# 4. Print out what's happening and convert model logits -> pred probs -> pred label\nprint(f\"Output logits:\\n{pred}\\n\")\nprint(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\nprint(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\nprint(f\"Actual label:\\n{label_single}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__p5G2ArcFko","outputId":"341f2b73-a0fa-4fdb-f458-142510dcfcca","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:44.041447Z","iopub.execute_input":"2025-04-19T06:18:44.041693Z","iopub.status.idle":"2025-04-19T06:18:44.257321Z","shell.execute_reply.started":"2025-04-19T06:18:44.041673Z","shell.execute_reply":"2025-04-19T06:18:44.256133Z"}},"outputs":[{"name":"stdout","text":"Single image shape: torch.Size([1, 3, 64, 64])\n\nOutput logits:\ntensor([[ 0.0208, -0.0020,  0.0095]])\n\nOutput prediction probabilities:\ntensor([[0.3371, 0.3295, 0.3333]])\n\nOutput prediction label:\ntensor([0])\n\nActual label:\n0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### 3.1 Making a model (TinyVGG) (script mode)\n\nOver the past few notebooks (notebook 03 and notebook 04), we've built the TinyVGG model a few times.\n\nSo it makes sense to put the model into its file so we can reuse it again and again.\n\nLet's put our `TinyVGG()` model class into a script called `model_builder.py` with the line `%%writefile going_modular/model_builder.py`. ","metadata":{"id":"wSaDm_W7bc3y"}},{"cell_type":"code","source":"%%writefile going_modular/model_builder.py\n\"\"\"\nContains PyTorch model code to instantiate a TinyVGG model.\n\"\"\"\nimport torch\n\nfrom torch import nn\n\nclass TinyVGG(nn.Module):\n    \"\"\"Creates the TinyVGG architecture.\n\n    Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n    See the original architecture here: https://poloclub.github.io/cnn-explainer/\n\n    Args:\n    input_shape: An integer indicating number of input channels.\n    hidden_units: An integer indicating number of hidden units between layers.\n    output_shape: An integer indicating number of output units.\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n          nn.Conv2d(in_channels=input_shape, \n                    out_channels=hidden_units, \n                    kernel_size=3, \n                    stride=1, \n                    padding=0),  \n          nn.ReLU(),\n          nn.Conv2d(in_channels=hidden_units, \n                    out_channels=hidden_units,\n                    kernel_size=3,\n                    stride=1,\n                    padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2,\n                        stride=2)\n        )\n        self.conv_block_2 = nn.Sequential(\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n          nn.Flatten(),\n          # Where did this in_features shape come from? \n          # It's because each layer of our network compresses and changes the shape of our inputs data.\n          nn.Linear(in_features=hidden_units*13*13,\n                    out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.conv_block_1(x)\n        x = self.conv_block_2(x)\n        x = self.classifier(x)\n        return x\n        # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oEH3bklL86hF","outputId":"b3f7e983-e51c-4e35-a991-df5ff99d63e6","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:44.258421Z","iopub.execute_input":"2025-04-19T06:18:44.258777Z","iopub.status.idle":"2025-04-19T06:18:44.268057Z","shell.execute_reply.started":"2025-04-19T06:18:44.258743Z","shell.execute_reply":"2025-04-19T06:18:44.267086Z"}},"outputs":[{"name":"stdout","text":"Writing going_modular/model_builder.py\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Create an instance of `TinyVGG` (from the script).","metadata":{"id":"3-WijBWM9K2c"}},{"cell_type":"code","source":"import torch\n\nfrom going_modular import model_builder\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Instantiate an instance of the model from the \"model_builder.py\" script\ntorch.manual_seed(42)\nmodel_1 = model_builder.TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                                hidden_units=10, \n                                output_shape=len(class_names)).to(device)\nmodel_1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F__hbQwZ9A56","outputId":"4e8abd04-cbc4-4aa8-b71c-3a8ff8617407","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:44.269223Z","iopub.execute_input":"2025-04-19T06:18:44.269653Z","iopub.status.idle":"2025-04-19T06:18:44.300127Z","shell.execute_reply.started":"2025-04-19T06:18:44.269624Z","shell.execute_reply":"2025-04-19T06:18:44.299304Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=1690, out_features=3, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"Do a dummy forward pass on `model_1`.","metadata":{"id":"vBD-yWcp9QVT"}},{"cell_type":"code","source":"# 1. Get a batch of images and labels from the DataLoader\nimg_batch, label_batch = next(iter(train_dataloader))\n\n# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\nimg_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\nprint(f\"Single image shape: {img_single.shape}\\n\")\n\n# 3. Perform a forward pass on a single image\nmodel_1.eval()\nwith torch.inference_mode():\n    pred = model_1(img_single.to(device))\n    \n# 4. Print out what's happening and convert model logits -> pred probs -> pred label\nprint(f\"Output logits:\\n{pred}\\n\")\nprint(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\nprint(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\nprint(f\"Actual label:\\n{label_single}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NCQWeeMj9kHE","outputId":"cb33c4ec-4848-423c-aea3-89a131e07cfd","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:44.301112Z","iopub.execute_input":"2025-04-19T06:18:44.301436Z","iopub.status.idle":"2025-04-19T06:18:44.386576Z","shell.execute_reply.started":"2025-04-19T06:18:44.301409Z","shell.execute_reply":"2025-04-19T06:18:44.385507Z"}},"outputs":[{"name":"stdout","text":"Single image shape: torch.Size([1, 3, 64, 64])\n\nOutput logits:\ntensor([[ 0.0208, -0.0020,  0.0095]])\n\nOutput prediction probabilities:\ntensor([[0.3371, 0.3295, 0.3333]])\n\nOutput prediction label:\ntensor([0])\n\nActual label:\n0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## 4. Creating `train_step()` and `test_step()` functions and `train()` to combine them  \n\nRather than writing them again, we can reuse the `train_step()` and `test_step()` functions from [notebook 04](https://www.learnpytorch.io/04_pytorch_custom_datasets/#75-create-train-test-loop-functions).\n\nThe same goes for the `train()` function we created.\n\nThe only difference here is that these functions have had docstrings added to them in [Google's Python Functions and Methods Style Guide](https://google.github.io/styleguide/pyguide.html#383-functions-and-methods).\n\nLet's start by making `train_step()`.","metadata":{"id":"8OB4xIsXcHrp"}},{"cell_type":"code","source":"from typing import Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -> Tuple[float, float]:\n    \"\"\"Trains a PyTorch model for a single epoch.\n\n    Turns a target PyTorch model to training mode and then\n    runs through all of the required training steps (forward\n    pass, loss calculation, optimizer step).\n\n    Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n    \"\"\"\n    # Put model in train mode\n    model.train()\n\n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n\n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc","metadata":{"id":"t_0zRYZecJZ6","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:44.387870Z","iopub.execute_input":"2025-04-19T06:18:44.388161Z","iopub.status.idle":"2025-04-19T06:18:44.395935Z","shell.execute_reply.started":"2025-04-19T06:18:44.388135Z","shell.execute_reply":"2025-04-19T06:18:44.394976Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"Now we'll do `test_step()`.","metadata":{"id":"1nwAPwNh-f3k"}},{"cell_type":"code","source":"def test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -> Tuple[float, float]:\n    \"\"\"Tests a PyTorch model for a single epoch.\n\n    Turns a target PyTorch model to \"eval\" mode and then performs\n    a forward pass on a testing dataset.\n\n    Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n    \"\"\"\n    # Put model in eval mode\n    model.eval() \n\n    # Setup test loss and test accuracy values\n    test_loss, test_acc = 0, 0\n\n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n\n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n\n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n    # Adjust metrics to get average loss and accuracy per batch \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc","metadata":{"id":"sr_9AspYcKkW","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:44.396883Z","iopub.execute_input":"2025-04-19T06:18:44.397462Z","iopub.status.idle":"2025-04-19T06:18:44.416285Z","shell.execute_reply.started":"2025-04-19T06:18:44.397433Z","shell.execute_reply":"2025-04-19T06:18:44.415316Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"And we'll combine `train_step()` and `test_step()` into `train()`.","metadata":{"id":"weqHDd5b-iGz"}},{"cell_type":"code","source":"from typing import Dict, List\n\nfrom tqdm.auto import tqdm\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -> Dict[str, List[float]]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n    # Return the filled results at the end of the epochs\n    return results","metadata":{"id":"-oze8b6icWE7","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:44.417385Z","iopub.execute_input":"2025-04-19T06:18:44.417709Z","iopub.status.idle":"2025-04-19T06:18:44.620764Z","shell.execute_reply.started":"2025-04-19T06:18:44.417683Z","shell.execute_reply":"2025-04-19T06:18:44.619864Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### 4.1 Creating `train_step()` and `test_step()` functions and `train()` to combine them (script mode)   \n\nTo create a script for `train_step()`, `test_step()` and `train()`, we'll combine their code all into a single cell.\n\nWe'll then write that cell to a file called `engine.py` because these functions will be the \"engine\" of our training pipeline.\n\nWe can do so with the magic line `%%writefile going_modular/engine.py`.\n\nWe'll also make sure to put all the imports we need (`torch`, `typing`, and `tqdm`) at the top of the cell.","metadata":{"id":"tw_nYa4kfQeI"}},{"cell_type":"code","source":"%%writefile going_modular/engine.py\n\"\"\"\nContains functions for training and testing a PyTorch model.\n\"\"\"\nfrom typing import Dict, List, Tuple\n\nimport torch\n\nfrom tqdm.auto import tqdm\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -> Tuple[float, float]:\n    \"\"\"Trains a PyTorch model for a single epoch.\n\n    Turns a target PyTorch model to training mode and then\n    runs through all of the required training steps (forward\n    pass, loss calculation, optimizer step).\n\n    Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n    \"\"\"\n    # Put model in train mode\n    model.train()\n\n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n\n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -> Tuple[float, float]:\n    \"\"\"Tests a PyTorch model for a single epoch.\n\n    Turns a target PyTorch model to \"eval\" mode and then performs\n    a forward pass on a testing dataset.\n\n    Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n    \"\"\"\n    # Put model in eval mode\n    model.eval() \n\n    # Setup test loss and test accuracy values\n    test_loss, test_acc = 0, 0\n\n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n\n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n\n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n    # Adjust metrics to get average loss and accuracy per batch \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -> Dict[str, List[float]]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n              train_acc: [...],\n              test_loss: [...],\n              test_acc: [...]} \n    For example if training for epochs=2: \n             {train_loss: [2.0616, 1.0537],\n              train_acc: [0.3945, 0.3945],\n              test_loss: [1.2641, 1.5706],\n              test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n    # Return the filled results at the end of the epochs\n    return results","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d47bhfY3fVq5","outputId":"41053838-2e28-4702-d833-d8c97eed119b","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:44.621766Z","iopub.execute_input":"2025-04-19T06:18:44.622093Z","iopub.status.idle":"2025-04-19T06:18:44.630778Z","shell.execute_reply.started":"2025-04-19T06:18:44.622066Z","shell.execute_reply":"2025-04-19T06:18:44.629884Z"}},"outputs":[{"name":"stdout","text":"Writing going_modular/engine.py\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## 5. Creating a function to save the model\n\nLet's setup a function to save our model to a directory.","metadata":{"id":"A2nXumITc9DL"}},{"cell_type":"code","source":"from pathlib import Path\n\ndef save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n    \"\"\"Saves a PyTorch model to a target directory.\n\n    Args:\n    model: A target PyTorch model to save.\n    target_dir: A directory for saving the model to.\n    model_name: A filename for the saved model. Should include\n      either \".pth\" or \".pt\" as the file extension.\n\n    Example usage:\n    save_model(model=model_0,\n               target_dir=\"models\",\n               model_name=\"05_going_modular_tingvgg_model.pth\")\n    \"\"\"\n    # Create target directory\n    target_dir_path = Path(target_dir)\n    target_dir_path.mkdir(parents=True,\n                        exist_ok=True)\n\n    # Create model save path\n    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n    model_save_path = target_dir_path / model_name\n\n    # Save the model state_dict()\n    print(f\"[INFO] Saving model to: {model_save_path}\")\n    torch.save(obj=model.state_dict(),\n             f=model_save_path)","metadata":{"id":"lu_vyjq-c933","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:44.631657Z","iopub.execute_input":"2025-04-19T06:18:44.632353Z","iopub.status.idle":"2025-04-19T06:18:44.653338Z","shell.execute_reply.started":"2025-04-19T06:18:44.632328Z","shell.execute_reply":"2025-04-19T06:18:44.652386Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### 5.1 Creating a function to save the model (script mode)\n\nHow about we add our `save_model()` function to a script called `utils.py` which is short for \"utilities\".\n\nWe can do so with the magic line `%%writefile going_modular/utils.py`.","metadata":{"id":"F7SnPGOfc_Wc"}},{"cell_type":"code","source":"%%writefile going_modular/utils.py\n\"\"\"\nContains various utility functions for PyTorch model training and saving.\n\"\"\"\nfrom pathlib import Path\n\nimport torch\n\ndef save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n    \"\"\"Saves a PyTorch model to a target directory.\n\n    Args:\n    model: A target PyTorch model to save.\n    target_dir: A directory for saving the model to.\n    model_name: A filename for the saved model. Should include\n      either \".pth\" or \".pt\" as the file extension.\n\n    Example usage:\n    save_model(model=model_0,\n               target_dir=\"models\",\n               model_name=\"05_going_modular_tingvgg_model.pth\")\n    \"\"\"\n    # Create target directory\n    target_dir_path = Path(target_dir)\n    target_dir_path.mkdir(parents=True,\n                        exist_ok=True)\n\n    # Create model save path\n    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n    model_save_path = target_dir_path / model_name\n\n    # Save the model state_dict()\n    print(f\"[INFO] Saving model to: {model_save_path}\")\n    torch.save(obj=model.state_dict(),\n             f=model_save_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20LC4xwN_2ZT","outputId":"30852196-8138-4f9e-9f43-578dc6277682","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:44.654305Z","iopub.execute_input":"2025-04-19T06:18:44.654712Z","iopub.status.idle":"2025-04-19T06:18:44.673578Z","shell.execute_reply.started":"2025-04-19T06:18:44.654683Z","shell.execute_reply":"2025-04-19T06:18:44.672617Z"}},"outputs":[{"name":"stdout","text":"Writing going_modular/utils.py\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## 6. Train, evaluate and save the model\n\nLet's leverage the functions we've got above to train, test and save a model to file.","metadata":{"id":"20ZO64U2cZY_"}},{"cell_type":"code","source":"# Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Recreate an instance of TinyVGG\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_0 \nmodel_0_results = train(model=model_0, \n                        train_dataloader=train_dataloader,\n                        test_dataloader=test_dataloader,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS,\n                        device=device)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n\n# Save the model\nsave_model(model=model_0,\n           target_dir=\"models\",\n           model_name=\"05_going_modular_cell_mode_tinyvgg_model.pth\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["84317b054c244ee4b1bed7b2f86660b7","a0a72aed7e2743f8a95c6af3b6e4a457","b517f855551f4c53969677161952a1ac","4d780e75e5d24241b1f6c0203a8bd73e","a0caf8f4760c4935b5b9be86566ee8e0","83cf71b79abd4fc5ae9daaac8f586841","3402f79d3df643efbed25c72aac94642","1e79615e722f456189e07874d13564d2","bac27d87b4d54b6cbaf85dd7b7ecdc64","f9b7fbae53e84883ac281343bb6a9da9","4bc100911d7d4303b6ab7efb59d3420c"]},"id":"FjeRMiLjccVc","outputId":"2d9005a4-afce-4230-c758-1c6efa01a5e8","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:44.674464Z","iopub.execute_input":"2025-04-19T06:18:44.674767Z","iopub.status.idle":"2025-04-19T06:18:55.439032Z","shell.execute_reply.started":"2025-04-19T06:18:44.674742Z","shell.execute_reply":"2025-04-19T06:18:55.438187Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122017ff3c114226b04d8fa6749712d6"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 | train_loss: 1.0926 | train_acc: 0.3956 | test_loss: 1.0728 | test_acc: 0.4133\nEpoch: 2 | train_loss: 1.0132 | train_acc: 0.4978 | test_loss: 0.9879 | test_acc: 0.4400\nEpoch: 3 | train_loss: 0.9836 | train_acc: 0.5511 | test_loss: 0.9838 | test_acc: 0.5067\nEpoch: 4 | train_loss: 0.9151 | train_acc: 0.5911 | test_loss: 0.9802 | test_acc: 0.4267\nEpoch: 5 | train_loss: 0.8840 | train_acc: 0.5822 | test_loss: 0.9771 | test_acc: 0.4667\n[INFO] Total training time: 10.741 seconds\n[INFO] Saving model to: models/05_going_modular_cell_mode_tinyvgg_model.pth\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### 6.1 Train, evaluate and save the model (script mode)\n\nLet's combine all of our modular files into a single script `train.py`.\n\nThis will allow us to run all of the functions we've written with a single line of code on the command line:\n\n`python going_modular/train.py`\n\nOr if we're running it in a notebook:\n\n`!python going_modular/train.py`\n\nWe'll go through the following steps:\n1. Import the various dependencies, namely `torch`, `os`, `torchvision.transforms` and all of the scripts from the `going_modular` directory, `data_setup`, `engine`, `model_builder`, `utils`.\n  * **Note:** Since `train.py` will be *inside* the `going_modular` directory, we can import the other modules via `import ...` rather than `from going_modular import ...`.\n2. Setup various hyperparameters such as batch size, number of epochs, learning rate and number of hidden units (these could be set in the future via [Python's `argparse`](https://docs.python.org/3/library/argparse.html)).\n3. Setup the training and test directories.\n4. Setup device-agnostic code.\n5. Create the necessary data transforms.\n6. Create the DataLoaders using `data_setup.py`.\n7. Create the model using `model_builder.py`.\n8. Setup the loss function and optimizer.\n9. Train the model using `engine.py`.\n10. Save the model using `utils.py`. ","metadata":{"id":"Md_CmXTvfkOz"}},{"cell_type":"code","source":"%%writefile going_modular/train.py\n\"\"\"\nTrains a PyTorch image classification model using device-agnostic code.\n\"\"\"\n\nimport os\n\nimport torch\n\nfrom torchvision import transforms\n\nimport data_setup, engine, model_builder, utils\n\n\n# Setup hyperparameters\nNUM_EPOCHS = 5\nBATCH_SIZE = 32\nHIDDEN_UNITS = 10\nLEARNING_RATE = 0.001\n\n# Setup directories\ntrain_dir = \"data/pizza_steak_sushi/train\"\ntest_dir = \"data/pizza_steak_sushi/test\"\n\n# Setup target device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Create transforms\ndata_transform = transforms.Compose([\n  transforms.Resize((64, 64)),\n  transforms.ToTensor()\n])\n\n# Create DataLoaders with help from data_setup.py\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=data_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Create model with help from model_builder.py\nmodel = model_builder.TinyVGG(\n    input_shape=3,\n    hidden_units=HIDDEN_UNITS,\n    output_shape=len(class_names)\n).to(device)\n\n# Set loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr=LEARNING_RATE)\n\n# Start training with help from engine.py\nengine.train(model=model,\n             train_dataloader=train_dataloader,\n             test_dataloader=test_dataloader,\n             loss_fn=loss_fn,\n             optimizer=optimizer,\n             epochs=NUM_EPOCHS,\n             device=device)\n\n# Save the model with help from utils.py\nutils.save_model(model=model,\n                 target_dir=\"models\",\n                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8XnG2l4zf2Ei","outputId":"84f67ba0-ddaf-45a2-9a71-a37cf3d1ca88","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:55.440023Z","iopub.execute_input":"2025-04-19T06:18:55.440314Z","iopub.status.idle":"2025-04-19T06:18:55.446501Z","shell.execute_reply.started":"2025-04-19T06:18:55.440286Z","shell.execute_reply":"2025-04-19T06:18:55.445738Z"}},"outputs":[{"name":"stdout","text":"Writing going_modular/train.py\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"Now our final directory structure looks like:\n```\ndata/\n  pizza_steak_sushi/\n    train/\n      pizza/\n        train_image_01.jpeg\n        train_image_02.jpeg\n        ...\n      steak/\n      sushi/\n    test/\n      pizza/\n        test_image_01.jpeg\n        test_image_02.jpeg\n        ...\n      steak/\n      sushi/\ngoing_modular/\n  data_setup.py\n  engine.py\n  model_builder.py\n  train.py\n  utils.py\nmodels/\n  saved_model.pth\n```\n\nNow to put it all together!\n\nLet's run our `train.py` file from the command line with:\n\n```\n!python going_modular/train.py\n```\n","metadata":{"id":"HHnPm-w7CjXT"}},{"cell_type":"code","source":"!python going_modular/train.py","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6eVtanbSCjFj","outputId":"9c90d0d8-9b44-4558-b74d-292729eca1e4","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:18:55.447439Z","iopub.execute_input":"2025-04-19T06:18:55.448021Z","iopub.status.idle":"2025-04-19T06:19:06.589595Z","shell.execute_reply.started":"2025-04-19T06:18:55.448000Z","shell.execute_reply":"2025-04-19T06:19:06.588671Z"}},"outputs":[{"name":"stdout","text":"  0%|                                                     | 0/5 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.0982 | train_acc: 0.4023 | test_loss: 1.0817 | test_acc: 0.5417\n 20%|█████████                                    | 1/5 [00:01<00:04,  1.17s/it]Epoch: 2 | train_loss: 1.0948 | train_acc: 0.2852 | test_loss: 1.0527 | test_acc: 0.5417\n 40%|██████████████████                           | 2/5 [00:02<00:03,  1.16s/it]Epoch: 3 | train_loss: 1.0767 | train_acc: 0.3203 | test_loss: 1.0712 | test_acc: 0.2708\n 60%|███████████████████████████                  | 3/5 [00:03<00:02,  1.15s/it]Epoch: 4 | train_loss: 1.0440 | train_acc: 0.4023 | test_loss: 1.0302 | test_acc: 0.3617\n 80%|████████████████████████████████████         | 4/5 [00:04<00:01,  1.14s/it]Epoch: 5 | train_loss: 1.0083 | train_acc: 0.4453 | test_loss: 0.9759 | test_acc: 0.4839\n100%|█████████████████████████████████████████████| 5/5 [00:05<00:00,  1.15s/it]\n[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"Woah!\n\nLook at that!\n\nWe've just trained a model with a single line of code from the command line.\n\nWe wrote a fair of code to do so, however, now we've got our code in `.py` files we can import them and reuse them as much as we like.\n\nFor exercises and extra-curriculum for this section, refer to the [online book version of 05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/#exercises).","metadata":{}}]}